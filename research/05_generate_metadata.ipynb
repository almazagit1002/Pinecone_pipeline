{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Generate_summaries_research_with_agents\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Maza\\\\Desktop\\\\Pinecone_pipeline\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Maza\\\\Desktop\\\\Pinecone_pipeline'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class JsonSummaryConfig:\n",
    "    \"\"\"\n",
    "    Configuration class for JSON summary settings.\n",
    "    \n",
    "    This class holds configuration settings required for handling JSON summaries.\n",
    "    It includes paths for directories and files as well as dictionaries for prompt \n",
    "    generation and model settings.\n",
    "\n",
    "    Attributes:\n",
    "        root_dir (Path): The root directory where all related files and directories are located.\n",
    "        read_schema (Path): The path to the schema file used for reading or validating JSON data.\n",
    "        load_json_summary (Path): The path where the JSON summary will be loaded from or saved to.\n",
    "        prompt_generate_json_summary (dict): A dictionary containing prompts used for generating JSON summaries.\n",
    "        models (dict): A dictionary containing model configurations for generating summaries.\n",
    "    \"\"\"\n",
    "    root_dir: Path\n",
    "    read_schema:Path\n",
    "    \n",
    "    prompt_generate_json_summary:dict\n",
    "    models: dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "from typing_extensions import TypedDict\n",
    "@dataclass(frozen=True)\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    This TypedDict class defines the structure for storing the state of a graph\n",
    "    during its processing. It includes details about the input file, intermediate\n",
    "    summaries, feedback, final summary, and other relevant metadata.\n",
    "\n",
    "    Attributes:\n",
    "        initial_file (str): The initial file path or name used as input for the graph.\n",
    "        draft_json_summary (dict): A dictionary holding the draft version of the JSON summary.\n",
    "        json_feedback (dict): A dictionary holding feedback for the JSON summary.\n",
    "        final_json_summary (dict): A dictionary holding the final version of the JSON summary.\n",
    "        num_steps (int): The number of steps or iterations taken in the graph processing.\n",
    "       \n",
    "    \"\"\"\n",
    "    initial_file : str\n",
    "    draft_json_summary : dict\n",
    "    json_feedback : dict\n",
    "    final_json_summary : dict\n",
    "    num_steps : int\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vector_db_pipeline.utils.common import load_json, read_yaml, create_directories,save_json\n",
    "from pathlib import Path\n",
    "from langchain_core.messages import  HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from vector_db_pipeline.constants import *\n",
    "from vector_db_pipeline import logger\n",
    "import json\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        models_filepath = MODELS_FILE_PATH,\n",
    "        prompt_template = PROMPT_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.models = read_yaml(models_filepath)\n",
    "        self.prompt_template = read_yaml(prompt_template)\n",
    "    \n",
    "        \n",
    "\n",
    "    \n",
    "    def get_json_summary_config(self) -> JsonSummaryConfig:\n",
    "        \"\"\"\n",
    "        Retrieves the configuration settings for JSON summary processing.\n",
    "\n",
    "        This method reads the configuration settings from the class instance's config attribute,\n",
    "        creates necessary directories, and initializes a JsonSummaryConfig object with the \n",
    "        retrieved settings.\n",
    "\n",
    "        Returns:\n",
    "            JsonSummaryConfig: An object containing configuration settings for JSON summary processing.\n",
    "        \"\"\"\n",
    "        config = self.config.json_summary\n",
    "        create_directories([config.root_dir])\n",
    "        \n",
    "        json_summary_config = JsonSummaryConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            read_schema = config.read_schema,\n",
    "            load_json_summary = config.load_json_summary,\n",
    "            prompt_generate_json_summary = self.prompt_template.generate_json_summary,\n",
    "            models = self.models,\n",
    "            \n",
    "        ) \n",
    "\n",
    "        return json_summary_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JsonSummary:\n",
    "    \"\"\"\n",
    "    A class to generate and manage JSON summaries using various agents and models.\n",
    "\n",
    "    Attributes:\n",
    "        config (JsonSummaryConfig): Configuration settings for JSON summary processing.\n",
    "    \"\"\"\n",
    "    def __init__(self, config:JsonSummaryConfig):\n",
    "        \"\"\"\n",
    "        Initializes the JsonSummary class with the provided configuration.\n",
    "\n",
    "        Args:\n",
    "            config (JsonSummaryConfig): Configuration settings for JSON summary processing.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "\n",
    "    \n",
    "    def files_in_app(self):\n",
    "        \"\"\"\n",
    "        Retrieves file paths from the directory structure specified in the schema.\n",
    "\n",
    "        This method reads a schema file, iterates through directories and subdirectories,\n",
    "        and collects all file paths mentioned in the schema.\n",
    "\n",
    "        Returns:\n",
    "            list[Path]: List of file paths extracted from the schema.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            schema = load_json(Path(self.config.read_schema))\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading schema file: {e}\")\n",
    "            return []\n",
    "\n",
    "        file_paths = []\n",
    "        \n",
    "        # Iterate through each directory in the schema\n",
    "        for directory, contents in schema.items():\n",
    "            if \"Files\" in contents:\n",
    "                # If the directory contains files\n",
    "                files = contents.get(\"Files\", [])\n",
    "                # Create full paths for each file in the directory\n",
    "                directory_path = Path(directory)\n",
    "                file_paths.extend([directory_path / file for file in files])\n",
    "            \n",
    "            if \"Subdirectories\" in contents:\n",
    "                # If the directory contains subdirectories, recursively call the function\n",
    "                try:\n",
    "                    subdirectory_paths = self.files_in_app(contents[\"Subdirectories\"])\n",
    "                    # Append subdirectory paths to the list\n",
    "                    file_paths.extend(subdirectory_paths)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing subdirectory: {e}\")\n",
    "\n",
    "        return file_paths\n",
    "    \n",
    "    def configure_model_system(self):\n",
    "        \"\"\"\n",
    "        Configures the model system with the appropriate agents and prompts.\n",
    "\n",
    "        This method sets up the models, templates, and agents required for generating,\n",
    "        validating, and editing JSON summaries.\n",
    "        \"\"\"\n",
    "        model =self.config.models.Llama3\n",
    "        logger.info(f\"Working with model: {model}\")\n",
    "        llm = ChatGroq(temperature=0, model_name=model)\n",
    "        \n",
    "        #Agents\n",
    "\n",
    "        # summary generator agent \n",
    "        json_creator_promt = PromptTemplate(\n",
    "            template= self.config.prompt_generate_json_summary.agent_summary_json_creator,\n",
    "            input_variables=[\"content\"])\n",
    "        \n",
    "        self.json_summary_generator = json_creator_promt | llm | JsonOutputParser()\n",
    "        \n",
    "        # analyse summary and decides if it has correct format\n",
    "        data_type_json_route_prompt = PromptTemplate(\n",
    "            template= self.config.prompt_generate_json_summary.agent_edit_json_route,\n",
    "            input_variables=[\"file\"])\n",
    "        \n",
    "        self.data_type_json_route_generator = data_type_json_route_prompt | llm | StrOutputParser()\n",
    "\n",
    "        # if json format is not correct this agent provides feedback for the final editor to aply corrections\n",
    "        json_feedbak_prompt =  PromptTemplate(\n",
    "            template= self.config.prompt_generate_json_summary.agent_feedbak_json,\n",
    "            input_variables=[\"file\"])\n",
    "        \n",
    "        self.json_feedbak_generator = json_feedbak_prompt | llm | JsonOutputParser()\n",
    "\n",
    "\n",
    "        # Final agent produce the final json object using the draft from self.json_summary_generator\n",
    "        # and the feedback from json_feedbak_generator\n",
    "        json_editor_prompt =  PromptTemplate(\n",
    "            template= self.config.prompt_generate_json_summary.agent_rewrite_json,\n",
    "            input_variables=[\"file\", \"feedback\"])\n",
    "        self.json_editor_generator = json_editor_prompt | llm | JsonOutputParser()\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    def generate_json_summary_agent(self,state):\n",
    "        \"\"\"\n",
    "        Generates a JSON summary draft from the initial file.\n",
    "\n",
    "        Args:\n",
    "            state (dict): The current state of the processing, including the initial file and step count.\n",
    "\n",
    "        Returns:\n",
    "            dict: Updated state with the draft JSON summary and incremented step count.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Creating JSON summary draft\")\n",
    "        initial_file= state['initial_file']\n",
    "        num_steps = int(state['num_steps'])\n",
    "        num_steps += 1\n",
    "\n",
    "        try:\n",
    "            draft_json_summary = self.json_summary_generator.invoke({\"content\": initial_file})\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in json summary generator agent: {e}\")\n",
    "        return {\"draft_json_summary\": draft_json_summary, \"num_steps\":num_steps}\n",
    "    \n",
    "    def json_format_route_agent(self, state):\n",
    "        \"\"\"\n",
    "        Determines if the JSON draft has the correct format.\n",
    "\n",
    "        Args:\n",
    "            state (dict): The current state of the processing, including the draft JSON summary and step count.\n",
    "\n",
    "        Returns:\n",
    "            str: 'edit_file' if the data type is incorrect, otherwise 'no_edit'.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Checking JSON draft data type\")\n",
    "        draft_json_summary = state.get('draft_json_summary')\n",
    "    \n",
    "        if draft_json_summary is None:\n",
    "            logger.error(\"No draft JSON summary provided.\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            summary_type = self.data_type_json_route_generator.invoke({\"file\": draft_json_summary})\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error determining JSON draft data type: {e}\")\n",
    "            return None\n",
    "\n",
    "        if summary_type == 'text':\n",
    "            return 'edit_file'\n",
    "        else:\n",
    "            return 'no_edit'\n",
    "        \n",
    "    def feedback_json(self,state):\n",
    "        \"\"\"\n",
    "        Provides feedback for correcting the JSON draft if the format is incorrect.\n",
    "\n",
    "        Args:\n",
    "            state (dict): The current state of the processing, including the draft JSON summary and step count.\n",
    "\n",
    "        Returns:\n",
    "            dict: Updated state with the feedback for the JSON draft and incremented step count.\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(f\"JSON draft incorrect.... Producing feedback \")\n",
    "        draft_json_summary= state['draft_json_summary']\n",
    "        num_steps = int(state['num_steps'])\n",
    "        num_steps += 1\n",
    "        try:\n",
    "            json_feedback = self.json_feedbak_generator.invoke({\"draft_json\": draft_json_summary})\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error producing fedback: {e}\")\n",
    "      \n",
    "        return {\"json_feedback\": json_feedback, \"num_steps\":num_steps}\n",
    "\n",
    "    \n",
    "    def no_rewrite(self,state):\n",
    "        \"\"\"\n",
    "        Finalizes the JSON draft as the final JSON summary if no corrections are needed.\n",
    "\n",
    "        Args:\n",
    "            state (dict): The current state of the processing, including the draft JSON summary and step count.\n",
    "\n",
    "        Returns:\n",
    "            dict: Updated state with the final JSON summary and incremented step count.\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(\"JSON summary draft correct and assigned to final_json_summary\")\n",
    "        ## Get the state\n",
    "        draft_json_summary = state[\"draft_json_summary\"]\n",
    "        num_steps = state['num_steps']\n",
    "        num_steps += 1\n",
    "\n",
    "        return {\"final_json_summary\": draft_json_summary, \"num_steps\":num_steps}\n",
    "    \n",
    "    \n",
    "\n",
    "    def edit_file_agent(self,state):\n",
    "        \"\"\"\n",
    "        Produces the final JSON summary using the draft and feedback.\n",
    "\n",
    "        Args:\n",
    "            state (dict): The current state of the processing, including the draft JSON summary, feedback, and step count.\n",
    "\n",
    "        Returns:\n",
    "            dict: Updated state with the final JSON summary and incremented step count.\n",
    "        \"\"\"\n",
    "        \n",
    "        logger.info(f\"Acting on feedback and producing final JSON summary\")\n",
    "        draft_json_summary= state['draft_json_summary']\n",
    "        json_feedback = state['json_feedback']\n",
    "        num_steps = int(state['num_steps'])\n",
    "        num_steps += 1\n",
    "        \n",
    "        try:\n",
    "            final_json_summary = self.json_editor_generator.invoke({\"file\": draft_json_summary, \"feedback\":json_feedback })\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error acting on fedback to generate final json file: {e}\")\n",
    "\n",
    "\n",
    "        return {\"final_json_summary\": final_json_summary, \"num_steps\":num_steps}\n",
    "       \n",
    "    def state_printer(self,state):\n",
    "        \"\"\"\n",
    "        Prints the current state of the processing.\n",
    "\n",
    "        Args:\n",
    "            state (dict): The current state of the processing.\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(\"---STATE PRINTER---\")\n",
    "        logger.info(f\"Initial_file: {state['initial_file']} \\n\" )\n",
    "        logger.info(f\"Draft Json Summary: {type(state['draft_json_summary'])} \\n\")\n",
    "        logger.info(f\"Final Json Summary: {type(state['final_json_summary'])} \\n\" )\n",
    "        logger.info(f\"Num Steps: {state['num_steps']} \\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    def create_graph_agents(self, workflow):\n",
    "        \"\"\"\n",
    "        Creates and configures the agent workflow graph.\n",
    "\n",
    "        Args:\n",
    "            workflow (Workflow): The workflow object to which nodes and edges will be added.\n",
    "\n",
    "        Returns:\n",
    "            CompiledWorkflow: The compiled workflow ready for execution.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        #nodes\n",
    "        workflow.add_node(\"generate_json_summary_agent\", self.generate_json_summary_agent) \n",
    "        workflow.add_node(\"feedback_json\", self.feedback_json)\n",
    "        workflow.add_node(\"edit_file_agent\", self.edit_file_agent)\n",
    "        workflow.add_node(\"no_rewrite\", self.no_rewrite)\n",
    "        workflow.add_node(\"state_printer\", self.state_printer)\n",
    "\n",
    "        #edges\n",
    "        workflow.set_entry_point(\"generate_json_summary_agent\")\n",
    "        workflow.add_conditional_edges(\n",
    "            \"generate_json_summary_agent\",\n",
    "            self.json_format_route_agent,\n",
    "            {\n",
    "                \"edit_file\": \"feedback_json\",\n",
    "                \"no_edit\": \"no_rewrite\",\n",
    "            },\n",
    "        )\n",
    "        workflow.add_edge(\"feedback_json\", \"edit_file_agent\")\n",
    "        workflow.add_edge(\"no_rewrite\", \"state_printer\")\n",
    "        workflow.add_edge(\"edit_file_agent\", \"state_printer\")\n",
    "        workflow.add_edge(\"state_printer\", END)\n",
    "\n",
    "        #compile\n",
    "        try:\n",
    "            app = workflow.compile()\n",
    "            logger.info(\"Agent graph created\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Agent graph error: {e}\")\n",
    "             \n",
    "        return  app\n",
    "    \n",
    "    def run_graph_agents(self, app,file_paths):\n",
    "        \"\"\"\n",
    "        Runs the agent workflow on a list of file paths to generate JSON summaries.\n",
    "\n",
    "        Args:\n",
    "            app (CompiledWorkflow): The compiled workflow to be executed.\n",
    "            file_paths (list[Path]): List of file paths to process.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        load_json_file = self.config.load_json_summary\n",
    "        logger.info(f\"Initializing agent summary orquestration\")\n",
    "        json_summaries = {}    \n",
    "\n",
    "\n",
    "        for file_path in file_paths:\n",
    "            logger.info(f\"Starting summary of {file_path}\")\n",
    "            encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'windows-1252']\n",
    "            for encoding in encodings:\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding=encoding) as f:\n",
    "                        file_content = f.read()\n",
    "                    logger.info(f\"File content successfully read using encoding: {encoding}\")\n",
    "                    break  # Stop trying encodings once successful\n",
    "                except UnicodeDecodeError:\n",
    "                    logger.error(f\"Failed to read file with encoding: {encoding}\")\n",
    "            \n",
    "            if len(file_content)>0:\n",
    "                filename = os.path.basename(file_path)\n",
    "                json_summaries[filename] = {}\n",
    "                json_summaries[filename]['FILE_PATH'] = str(file_path)\n",
    "                \n",
    "                inputs = {\"initial_file\": file_content,\"num_steps\":0}\n",
    "                try:\n",
    "                    output = app.invoke(inputs)\n",
    "                except:\n",
    "                    logger.error(f\"Error in agent graph: {e}\")\n",
    "                \n",
    "                for key, value in output['final_json_summary'].items():\n",
    "                    json_summaries[filename][key] = value\n",
    "                logger.info(f\"{filename} summary success\")\n",
    "            else:\n",
    "                logger.info(f\"{filename} empty.\")\n",
    "\n",
    "        try:\n",
    "            save_json(Path(load_json_file), json_summaries)\n",
    "            logger.info(f\"JSON summaries loaded to {load_json_file}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving JSON summaries: {e}\")\n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-14 14:25:10,619: INFO: common: yaml file: config\\config.yaml loaded successfully:]\n",
      "[2024-05-14 14:25:10,622: INFO: common: yaml file: schema.yaml loaded successfully:]\n",
      "[2024-05-14 14:25:10,626: INFO: common: yaml file: params.yaml loaded successfully:]\n",
      "[2024-05-14 14:25:10,628: INFO: common: yaml file: models.yaml loaded successfully:]\n",
      "[2024-05-14 14:25:10,634: INFO: common: yaml file: prompt_template.yaml loaded successfully:]\n",
      "[2024-05-14 14:25:10,635: INFO: common: Directory already exists: artifacts/json_summary:]\n",
      "[2024-05-14 14:25:10,638: INFO: common: json file loaded succesfully from: artifacts\\app_schema\\schema.json:]\n",
      "[2024-05-14 14:25:10,640: INFO: 1977102676: Working with model: llama3-70b-8192:]\n",
      "[2024-05-14 14:25:11,642: INFO: 1977102676: Agent graph created:]\n",
      "[2024-05-14 14:25:11,642: INFO: 1977102676: Initializing agent summary orquestration:]\n",
      "[2024-05-14 14:25:11,642: INFO: 1977102676: Starting summary of .gitignore:]\n",
      "[2024-05-14 14:25:11,642: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:25:11,733: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:25:13,403: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:25:13,424: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:25:14,836: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:25:14,852: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:25:14,873: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:25:14,873: INFO: 1977102676: Initial_file: # Byte-compiled / optimized / DLL files\n",
      "__pycache__/\n",
      "*.py[cod]\n",
      "*$py.class\n",
      "\n",
      "# C extensions\n",
      "*.so\n",
      "\n",
      "# Distribution / packaging\n",
      ".Python\n",
      "build/\n",
      "develop-eggs/\n",
      "dist/\n",
      "downloads/\n",
      "eggs/\n",
      ".eggs/\n",
      "lib/\n",
      "lib64/\n",
      "parts/\n",
      "sdist/\n",
      "var/\n",
      "wheels/\n",
      "share/python-wheels/\n",
      "*.egg-info/\n",
      ".installed.cfg\n",
      "*.egg\n",
      "MANIFEST\n",
      "\n",
      "# PyInstaller\n",
      "#  Usually these files are written by a python script from a template\n",
      "#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n",
      "*.manifest\n",
      "*.spec\n",
      "\n",
      "# Installer logs\n",
      "pip-log.txt\n",
      "pip-delete-this-directory.txt\n",
      "\n",
      "# Unit test / coverage reports\n",
      "htmlcov/\n",
      ".tox/\n",
      ".nox/\n",
      ".coverage\n",
      ".coverage.*\n",
      ".cache\n",
      "nosetests.xml\n",
      "coverage.xml\n",
      "*.cover\n",
      "*.py,cover\n",
      ".hypothesis/\n",
      ".pytest_cache/\n",
      "cover/\n",
      "\n",
      "# Translations\n",
      "*.mo\n",
      "*.pot\n",
      "\n",
      "# Django stuff:\n",
      "*.log\n",
      "local_settings.py\n",
      "db.sqlite3\n",
      "db.sqlite3-journal\n",
      "\n",
      "# Flask stuff:\n",
      "instance/\n",
      ".webassets-cache\n",
      "\n",
      "# Scrapy stuff:\n",
      ".scrapy\n",
      "\n",
      "# Sphinx documentation\n",
      "docs/_build/\n",
      "\n",
      "# PyBuilder\n",
      ".pybuilder/\n",
      "target/\n",
      "\n",
      "# Jupyter Notebook\n",
      ".ipynb_checkpoints\n",
      "\n",
      "# IPython\n",
      "profile_default/\n",
      "ipython_config.py\n",
      "\n",
      "# pyenv\n",
      "#   For a library or package, you might want to ignore these files since the code is\n",
      "#   intended to run in multiple environments; otherwise, check them in:\n",
      "# .python-version\n",
      "\n",
      "# pipenv\n",
      "#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n",
      "#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n",
      "#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n",
      "#   install all needed dependencies.\n",
      "#Pipfile.lock\n",
      "\n",
      "# poetry\n",
      "#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n",
      "#   This is especially recommended for binary packages to ensure reproducibility, and is more\n",
      "#   commonly ignored for libraries.\n",
      "#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n",
      "#poetry.lock\n",
      "\n",
      "# pdm\n",
      "#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n",
      "#pdm.lock\n",
      "#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n",
      "#   in version control.\n",
      "#   https://pdm.fming.dev/#use-with-ide\n",
      ".pdm.toml\n",
      "\n",
      "# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n",
      "__pypackages__/\n",
      "\n",
      "# Celery stuff\n",
      "celerybeat-schedule\n",
      "celerybeat.pid\n",
      "\n",
      "# SageMath parsed files\n",
      "*.sage.py\n",
      "\n",
      "# Environments\n",
      ".env\n",
      ".venv\n",
      "env/\n",
      "venv/\n",
      "ENV/\n",
      "env.bak/\n",
      "venv.bak/\n",
      "\n",
      "# Spyder project settings\n",
      ".spyderproject\n",
      ".spyproject\n",
      "\n",
      "# Rope project settings\n",
      ".ropeproject\n",
      "\n",
      "# mkdocs documentation\n",
      "/site\n",
      "\n",
      "# mypy\n",
      ".mypy_cache/\n",
      ".dmypy.json\n",
      "dmypy.json\n",
      "\n",
      "# Pyre type checker\n",
      ".pyre/\n",
      "\n",
      "# pytype static type analyzer\n",
      ".pytype/\n",
      "\n",
      "# Cython debug symbols\n",
      "cython_debug/\n",
      "\n",
      "# PyCharm\n",
      "#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n",
      "#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n",
      "#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n",
      "#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n",
      "#.idea/\n",
      "\n",
      "artifacts/*\n",
      "logs/*\n",
      "research/logs/*\n",
      "src/vector_db_pipeline/pipeline/logs/*\n",
      " \n",
      ":]\n",
      "[2024-05-14 14:25:14,873: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:25:14,873: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:25:14,873: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:25:14,873: INFO: 1977102676: .gitignore summary success:]\n",
      "[2024-05-14 14:25:14,887: INFO: 1977102676: Starting summary of data.yaml:]\n",
      "[2024-05-14 14:25:14,889: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:25:14,914: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:25:15,426: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:25:15,442: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:25:15,790: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:25:15,806: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:25:15,825: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:25:15,825: INFO: 1977102676: Initial_file: my_set: !!set\n",
      "  .git: null\n",
      "  .github: null\n",
      "  Data: null\n",
      "  LICENSE: null\n",
      "  README.md: null\n",
      " \n",
      ":]\n",
      "[2024-05-14 14:25:15,825: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:25:15,825: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:25:15,825: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:25:15,837: INFO: 1977102676: data.yaml summary success:]\n",
      "[2024-05-14 14:25:15,838: INFO: 1977102676: Starting summary of Dockerfile:]\n",
      "[2024-05-14 14:25:15,841: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:25:15,857: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:25:16,401: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:25:16,431: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:25:17,971: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:25:17,992: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:25:18,006: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:25:18,007: INFO: 1977102676: Initial_file: FROM python:3.8-slim-buster\n",
      "\n",
      "RUN apt update -y\n",
      "WORKDIR /pinecone_pipeline\n",
      "\n",
      "COPY . /pinecone_pipeline\n",
      "RUN pip install -r requirements.txt\n",
      "\n",
      "CMD [\"python3\", \"main.py\"] \n",
      ":]\n",
      "[2024-05-14 14:25:18,008: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:25:18,009: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:25:18,009: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:25:18,015: INFO: 1977102676: Dockerfile summary success:]\n",
      "[2024-05-14 14:25:18,015: INFO: 1977102676: Starting summary of exhalation.py:]\n",
      "[2024-05-14 14:25:18,021: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:25:18,041: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:25:18,773: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:25:18,789: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:25:19,291: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:25:19,313: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:25:19,328: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:25:19,328: INFO: 1977102676: Initial_file: from vector_db_pipeline import logger\n",
      "from vector_db_pipeline.pipeline.GenerateAppStructure import GenerateAppStructurePipeline\n",
      "\n",
      "STAGE_NAME = \"Getting App File Structure stage\"\n",
      "\n",
      "try:\n",
      "    logger.info(f\">>>>>>> stage {STAGE_NAME} started <<<<<<<<<<<<\")\n",
      "    obj = GenerateAppStructurePipeline()\n",
      "    obj.main()\n",
      "    logger.info(f\">>>>>>> stage {STAGE_NAME} completed <<<<<<<<<<<<\\n\\nx===============x\")\n",
      "\n",
      "except Exception as e:\n",
      "    logger.exception(e)\n",
      "    raise(e) \n",
      ":]\n",
      "[2024-05-14 14:25:19,329: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:25:19,330: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:25:19,331: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:25:19,340: INFO: 1977102676: exhalation.py summary success:]\n",
      "[2024-05-14 14:25:19,342: INFO: 1977102676: Starting summary of exhalation_ignore.yaml:]\n",
      "[2024-05-14 14:25:19,342: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:25:19,362: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:25:21,091: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:25:21,107: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:25:22,457: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:25:22,478: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:25:22,492: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:25:22,493: INFO: 1977102676: Initial_file: IGNORE_FILES: !!set\n",
      "  .git: null\n",
      "  .github: null\n",
      "  Data: null\n",
      "  README.md: null \n",
      "  LICENSE: null\n",
      "  research: null\n",
      "  requirements.txt: null \n",
      ":]\n",
      "[2024-05-14 14:25:22,494: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:25:22,495: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:25:22,496: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:25:22,506: INFO: 1977102676: exhalation_ignore.yaml summary success:]\n",
      "[2024-05-14 14:25:22,507: INFO: 1977102676: Starting summary of main.py:]\n",
      "[2024-05-14 14:25:22,507: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:25:22,529: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:25:23,342: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:25:23,385: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:25:23,940: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:25:23,958: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:25:23,981: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:25:23,983: INFO: 1977102676: Initial_file: from vector_db_pipeline import logger\n",
      "from vector_db_pipeline.pipeline.DataIngestion import DataIngestionPipeline\n",
      "from vector_db_pipeline.pipeline.DataValidation import DataValidationPipeline\n",
      "from vector_db_pipeline.pipeline.DataUpload import DataUploadPipeline\n",
      "from vector_db_pipeline.utils.common import read_yaml\n",
      "from vector_db_pipeline.constants import *\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STAGE_NAME = \"Data Ingestion stage\"\n",
      "try:\n",
      "    logger.info(f\">>>>>>> stage {STAGE_NAME} started <<<<<<<<<<<<\")\n",
      "    data_ingestion = DataIngestionPipeline()\n",
      "    data_ingestion.main()\n",
      "    logger.info(f\">>>>>>> stage {STAGE_NAME} completed <<<<<<<<<<<<\\n\\nx===============x\")\n",
      "\n",
      "except Exception as e:\n",
      "    logger.error(e)\n",
      "    raise(e)\n",
      "\n",
      "\n",
      "STAGE_NAME = \"Data Validation stage\"\n",
      "\n",
      "try:\n",
      "    logger.info(f\">>>>>>> stage {STAGE_NAME} started <<<<<<<<<<<<\")\n",
      "    data_val = DataValidationPipeline()\n",
      "    data_val.main()\n",
      "    logger.info(f\">>>>>>> stage {STAGE_NAME} completed <<<<<<<<<<<<\\n\\nx===============x\")\n",
      "\n",
      "except Exception as e:\n",
      "    logger.error(e)\n",
      "    raise(e)\n",
      "\n",
      "STAGE_NAME = \"Data Upload stage\"\n",
      "\n",
      "try: \n",
      "    params = read_yaml(PARAMS_FILE_PATH)\n",
      "    delete_vector_database = params.DELETE_DATABSE.DELETE_DATABSE\n",
      "    logger.info(f\">>>>>>> stage {STAGE_NAME} started <<<<<<<<<<<<\")\n",
      "    data_upload = DataUploadPipeline(should_restart_database=delete_vector_database)\n",
      "    data_upload.main()\n",
      "    logger.info(f\">>>>>>> stage {STAGE_NAME} completed <<<<<<<<<<<<\\n\\nx===============x\")\n",
      "\n",
      "except Exception as e:\n",
      "    logger.error(e)\n",
      "    raise(e) \n",
      ":]\n",
      "[2024-05-14 14:25:23,986: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:25:23,989: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:25:23,992: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:25:24,009: INFO: 1977102676: main.py summary success:]\n",
      "[2024-05-14 14:25:24,011: INFO: 1977102676: Starting summary of models.yaml:]\n",
      "[2024-05-14 14:25:24,013: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:25:24,122: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:25:24,730: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:25:24,753: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:25:26,685: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:25:26,706: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:25:26,718: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:25:26,719: INFO: 1977102676: Initial_file: Llama3: llama3-70b-8192\n",
      "Mistral: mixtral-8x7b-32768\n",
      "\n",
      " \n",
      ":]\n",
      "[2024-05-14 14:25:26,720: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:25:26,722: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:25:26,723: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:25:26,732: INFO: 1977102676: models.yaml summary success:]\n",
      "[2024-05-14 14:25:26,734: INFO: 1977102676: Starting summary of params.yaml:]\n",
      "[2024-05-14 14:25:26,737: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:25:26,758: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:25:28,082: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:25:28,099: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:25:28,700: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:25:28,720: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:25:28,725: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:25:28,725: INFO: 1977102676: Initial_file: TEXT_SPLITER:\n",
      "  SEPARATOR: \\n\n",
      "  CHUNK_SIZE: 1000\n",
      "  CHUNK_OVERLAP: 200\n",
      "\n",
      "\n",
      "INDEX_INFO:\n",
      "  INDEX_NAME: meshlennysnews\n",
      "  DIMENSIONS: 1536\n",
      "  METRIC: cosine\n",
      "  ENVIROMENT: gcp-starter\n",
      "  NAMESPACE: 'blog'\n",
      "\n",
      "BATCH_SIZE:\n",
      "  BATCH_SIZE: 120\n",
      "\n",
      "DELETE_DATABSE:\n",
      "  DELETE_DATABSE: True\n",
      "\n",
      " \n",
      ":]\n",
      "[2024-05-14 14:25:28,725: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:25:28,725: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:25:28,737: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:25:28,743: INFO: 1977102676: params.yaml summary success:]\n",
      "[2024-05-14 14:25:28,743: INFO: 1977102676: Starting summary of prompt_template.yaml:]\n",
      "[2024-05-14 14:25:28,743: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:25:28,761: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:25:32,125: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:25:32,142: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:25:32,972: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:25:32,992: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:25:33,004: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:25:33,005: INFO: 1977102676: Initial_file: generate_file_structure:\n",
      "  description: |\n",
      "    Given a JSON file representing a directory structure, generate a text representation of the directory structure.\n",
      "    \n",
      "    The JSON file has the following structure:\n",
      "     ```\n",
      "    JSON_FILE:\n",
      "\n",
      "    {JSON_FILE}\n",
      "    ```\n",
      "\n",
      "    Where `JSON_FILE` contains the directory structure data in JSON format, specifying the directories, subdirectories, and files.\n",
      "\n",
      "    The text representation should follow this format:\n",
      "\n",
      "    ```\n",
      "    |__ directory1/\n",
      "        |__ subdirectory1/\n",
      "            |__ file1\n",
      "            |__ file2\n",
      "        |__ subdirectory2/\n",
      "            |__ file3\n",
      "    |__ directory2/\n",
      "        |__ file4\n",
      "        |__ file5\n",
      "    ```\n",
      "\n",
      "    Each directory and subdirectory should be represented by \"|__\", and files should be listed underneath their respective directories.\n",
      "\n",
      "    Ensure that the generated text representation accurately reflects the directory structure specified in the JSON file.\n",
      "\n",
      "    Ensure you include the \"Files\" in the root directory.\n",
      "\n",
      "    RETURN ONLY THE TEXT REPRESENTATION, NO SCRIPTS, NO COMMENTS, NO EXPLANATIONS.\n",
      "\n",
      "generate_graph_structure:\n",
      "    system: |\n",
      "            You are a data modelling expert capable of creating high quality entity-relationship models from denormalised datasets. \n",
      "            You always follow these modeling principles: \n",
      "            You don't overnormalize the model. \n",
      "            You don't use the same name for realtionships connecting different types of entities. \n",
      "            You make sure that all features in the dataset are included in the model. \n",
      "            You make sure there is a one to one mapping between the attributes in the extracted entities and the features in the dataset provided as input. \n",
      "    task: |\n",
      "           From the following app file structure below,\n",
      "            extract a list of entities and relationships with their attributes and map them to the features.\n",
      "            The attributes don't need to be named after the features in the dataset, but they should be mapped to the corresponding feature name. \n",
      "            Give meaningful names to relationships rather than just a pair of entities. \n",
      "            In addition to that, for each entity, attribute and relationship, provide the closest schema.org term identified by its uri. \n",
      "            The output format should be as follows:  \n",
      "            {'entities': [ { 'name': 'the entity name', schema_org_term: 'uri of the closest schema.org term for the entity', 'attributes' : [ { 'name': 'the att name', schema_org_term: 'uri of the closest schema.org term for the attribute', 'mappedTo' : 'the feature name'}, ...]}, 'relationships': [ { 'name': 'the entity name', schema_org_term: 'uri of the closest schema.org term for the relationship', 'from': 'source entity', 'to': 'target entity'}]}  \n",
      "            RETURN ONLY THE JSON REPRESENTATION, NO SCRIPTS, NO COMMENTS, NO EXPLANATIONS , MAKE SURE TO CLOSE ALL OPEN BRAKETS (TYPES '())','[]' AND '{}')!!!.\n",
      "\n",
      "\n",
      "    system_debugger: You are a code tester, and your code has a bug that YOU NEED TO FIX!!!\n",
      "    \n",
      "    task_debugger: |\n",
      "        Given the error:\n",
      "        \n",
      "        {error}\n",
      "        \n",
      "        Edit the following json file:\n",
      "\n",
      "        {results}.\n",
      "        \n",
      "        THERE IS DEFENITELY AN ERROR!!! so rewrite the json file line by line \n",
      "        making sure every key value pair is properly matched and there is no open parentheisis.\n",
      "        Rewrite the json file line by line with the proper indentations.\n",
      "        RETURN ONLY THE JSON REPRESENTATION, NO SCRIPTS, NO COMMENTS, NO EXPLANATIONS!!!!!!\n",
      "\n",
      "    \n",
      "generate_json_summary:\n",
      "    \n",
      "    agent_summary_json_creator: |\n",
      "        <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "        You are an expert software developer.\n",
      "        You will read CONTENT with, code, yaml files, docker files, txt, json, etc.\n",
      "        You summarize what the code does, list the functions, classes and methods.\n",
      "        You will have to understand the dependecies from each file.\n",
      "        Return JSON with the relevant KEYS and no premable or explaination.\n",
      "        KEYS: \n",
      "            SUMMARY: make a summary of the content.\n",
      "            CLASSES: list of the classes and functionality, if there is no classes DON NOT INCLUEDE THIS KEY.\n",
      "            METHODS: list of the methods and functionality, if there is no methods DON NOT INCLUEDE THIS KEY. \n",
      "            FUNCTIONS: list of functions and functionality, if there is no functions DON NOT INCLUDE THIS KEY.\n",
      "            LIBRARIES: list of libraries, if there is no libraries DO NOT INCLUDE THIS KEY.\n",
      "            DEPENDENCIES: List of custom dependencies, if there is no dependencies DO NOT INCLUDE THIS KEY.\n",
      "\n",
      "        <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "        CONTENT:\n",
      "            {content}\n",
      "        RETURN ONLY THE JSON, ABSOLUTELY NO SCRIPTS, NO COMMENTS, NO EXPLANATIONS AND NO PREAMBLE!!!\n",
      "        <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "    agent_edit_json_route: |\n",
      "        <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "        You are an expert at identifying file_types.\n",
      "        <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "        Using the provided FILE, determine if the format is a json or it includes additional text.\n",
      "         Output a single cetgory only from the 2 options:\n",
      "          json or text\n",
      "          eg:\n",
      "            text\n",
      "        FILE:\n",
      "            {file}\n",
      "        <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "        \n",
      "\n",
      "    agent_feedbak_json: |\n",
      "        <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "        You are the Quality Control Agent read the DRAFT_JSON file and give a description\n",
      "        on what to edit to generate a json file only, with no extra text, and fromated correctly.\n",
      "\n",
      "        <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "        DRAFT_JSON:\n",
      "            {draft_json}\n",
      "        \n",
      "        Return the analysis a JSON with a single key 'draft_analysis' and no premable or explaination.\n",
      "        \n",
      "        <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "    agent_rewrite_json: |\n",
      "        <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "        You are the Final json summary Agent read the FILE \n",
      "        and and FEDBACK and edit the FILE acordingly.\n",
      "        Return the FILE as JSON no premable or explaination.\n",
      "\n",
      "        <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "        FEDBACK:\n",
      "            {feedback}\n",
      "        FILE:\n",
      "            {file}\n",
      "        \n",
      "        <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      " \n",
      ":]\n",
      "[2024-05-14 14:25:33,006: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:25:33,007: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:25:33,009: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:25:33,016: INFO: 1977102676: prompt_template.yaml summary success:]\n",
      "[2024-05-14 14:25:33,016: INFO: 1977102676: Starting summary of schema.yaml:]\n",
      "[2024-05-14 14:25:33,020: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:25:33,043: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:25:33,705: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:25:33,730: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:25:34,100: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:25:34,121: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:25:34,126: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:25:34,126: INFO: 1977102676: Initial_file: COLUMNS:\n",
      "  date_scraped_timestamp: int\n",
      "  host: str\n",
      "  page_title: str\n",
      "  text: str\n",
      "  url: str \n",
      ":]\n",
      "[2024-05-14 14:25:34,126: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:25:34,137: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:25:34,138: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:25:34,142: INFO: 1977102676: schema.yaml summary success:]\n",
      "[2024-05-14 14:25:34,142: INFO: 1977102676: Starting summary of setup.py:]\n",
      "[2024-05-14 14:25:34,153: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:25:34,174: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:25:34,827: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:25:34,838: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:25:35,205: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:25:35,240: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:25:35,242: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:25:35,242: INFO: 1977102676: Initial_file: import setuptools\n",
      "\n",
      "with open(\"README.md\", \"r\", encoding=\"utf-8\") as f:\n",
      "    long_description = f.read()\n",
      "\n",
      "\n",
      "__version__ = \"0.0.1\"\n",
      "\n",
      "REPO_NAME = \"Pinecone_pipeline\"\n",
      "AUTHOR_USER_NAME = \"almazagit1002\"\n",
      "SRC_REPO = \"Pinecone_pipeline\"\n",
      "AUTHOR_EMAIL = \"almazagit1002@gmail.com\"\n",
      "\n",
      "\n",
      "setuptools.setup(\n",
      "    name=SRC_REPO,\n",
      "    version=__version__,\n",
      "    author=AUTHOR_USER_NAME,\n",
      "    author_email=AUTHOR_EMAIL,\n",
      "    description=\"A text to vector bd pipeline\",\n",
      "    long_description=long_description,\n",
      "    long_description_content_type=\"text/markdown\",  # Corrected option name\n",
      "    url=f\"https://github.com/{AUTHOR_USER_NAME}/{REPO_NAME}\",\n",
      "    project_urls={\n",
      "        \"Bug Tracker\": f\"https://github.com/{AUTHOR_USER_NAME}/{REPO_NAME}/issues\",\n",
      "    },\n",
      "    package_dir={\"\": \"src\"},\n",
      "    packages=setuptools.find_packages(where=\"src\")\n",
      ")\n",
      " \n",
      ":]\n",
      "[2024-05-14 14:25:35,253: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:25:35,254: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:25:35,255: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:25:35,264: INFO: 1977102676: setup.py summary success:]\n",
      "[2024-05-14 14:25:35,265: INFO: 1977102676: Starting summary of structure.txt:]\n",
      "[2024-05-14 14:25:35,274: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:25:35,297: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:25:36,489: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:25:36,509: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:25:36,722: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:25:36,722: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 1.000000 seconds:]\n",
      "[2024-05-14 14:25:38,637: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:25:38,658: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:25:38,660: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:25:38,670: INFO: 1977102676: Initial_file: |__ .\n",
      "    |__ config/\n",
      "        |__ config.yaml\n",
      "    |__ Data/\n",
      "        |__ data_productplan_blog.json\n",
      "        |__ data_romanpichler_blog.json\n",
      "        |__ data_svpg_blog.json\n",
      "    |__ research/\n",
      "        |__ 01_data_ingestion.ipynb\n",
      "        |__ 02_data_validation.ipynb\n",
      "        |__ 03_data_load.ipynb\n",
      "        |__ 04_generate_app_schema.ipynb\n",
      "        |__ trials.ipynb\n",
      "    |__ src/\n",
      "        |__ vector_db_pipeline/\n",
      "            |__ components/\n",
      "                |__ data_ingestion.py\n",
      "                |__ data_load.py\n",
      "                |__ data_validation.py\n",
      "                |__ generate_app_structure.py\n",
      "                |__ __init__.py\n",
      "            |__ config/\n",
      "                |__ configuration.py\n",
      "                |__ __init__.py\n",
      "            |__ constants/\n",
      "                |__ __init__.py\n",
      "            |__ entity/\n",
      "                |__ config_entity.py\n",
      "                |__ __init__.py\n",
      "            |__ pipeline/\n",
      "                |__ DataIngestion.py\n",
      "                |__ DataUpload.py\n",
      "                |__ DataValidation.py\n",
      "                |__ GenerateAppStructure.py\n",
      "                |__ __init__.py\n",
      "            |__ utils/\n",
      "                |__ common.py\n",
      "                |__ __init__.py\n",
      "            |__ __init__.py\n",
      "    |__ .gitignore\n",
      "    |__ Dockerfile\n",
      "    |__ exhalation.py\n",
      "    |__ knowledge_graph.txt\n",
      "    |__ LICENSE\n",
      "    |__ main.py\n",
      "    |__ models.yaml\n",
      "    |__ params.yaml\n",
      "    |__ prompt_template.yaml\n",
      "    |__ README.md\n",
      "    |__ requirements.txt\n",
      "    |__ schema.yaml\n",
      "    |__ setup.py\n",
      "    |__ structure.txt\n",
      "    |__ Summary.txt\n",
      "    |__ template.py \n",
      ":]\n",
      "[2024-05-14 14:25:38,671: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:25:38,672: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:25:38,673: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:25:38,682: INFO: 1977102676: structure.txt summary success:]\n",
      "[2024-05-14 14:25:38,682: INFO: 1977102676: Starting summary of template.py:]\n",
      "[2024-05-14 14:25:38,693: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:25:38,708: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:25:38,938: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:25:38,938: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 4.000000 seconds:]\n",
      "[2024-05-14 14:25:43,550: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:25:43,570: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:25:43,777: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:25:43,777: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 1.000000 seconds:]\n",
      "[2024-05-14 14:25:45,381: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:25:45,402: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:25:45,415: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:25:45,416: INFO: 1977102676: Initial_file: import os\n",
      "from pathlib import Path\n",
      "import logging\n",
      "\n",
      "logging.basicConfig(level=logging.INFO, format='[%(asctime)s]: %(message)s:')\n",
      "\n",
      "project_name = \"vector_db_pipeline\"\n",
      "\n",
      "\n",
      "list_of_files = [\n",
      "    \".github/workflows/.gitkeep\",\n",
      "    f\"src/{project_name}/__init__.py\",\n",
      "    f\"src/{project_name}/components/__init__.py\",\n",
      "    f\"src/{project_name}/utils/__init__.py\",\n",
      "    f\"src/{project_name}/utils/common.py\",\n",
      "    f\"src/{project_name}/config/__init__.py\",\n",
      "    f\"src/{project_name}/config/configuration.py\",\n",
      "    f\"src/{project_name}/pipeline/__init__.py\",\n",
      "    f\"src/{project_name}/entity/__init__.py\",\n",
      "    f\"src/{project_name}/entity/config_entity.py\",\n",
      "    f\"src/{project_name}/constants/__init__.py\",\n",
      "    \"config/config.yaml\",\n",
      "    \"params.yaml\",\n",
      "    \"schema.yaml\",\n",
      "    \"main.py\",\n",
      "    \"app.py\",\n",
      "    \"Dockerfile\",\n",
      "    \"setup.py\",\n",
      "    \"research/trials.ipynb\"\n",
      "\n",
      "\n",
      "]\n",
      "\n",
      "\n",
      "for filepath in list_of_files:\n",
      "    filepath = Path(filepath)\n",
      "    filedir, filename = os.path.split(filepath)\n",
      "\n",
      "\n",
      "    if filedir !=\"\":\n",
      "        os.makedirs(filedir, exist_ok=True)\n",
      "        logging.info(f\"Creating directory; {filedir} for the file: {filename}\")\n",
      "\n",
      "    if (not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\n",
      "        with open(filepath, \"w\") as f:\n",
      "            pass\n",
      "            logging.info(f\"Creating empty file: {filepath}\")\n",
      "\n",
      "\n",
      "    else:\n",
      "        logging.info(f\"{filename} is already exists\") \n",
      ":]\n",
      "[2024-05-14 14:25:45,417: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:25:45,418: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:25:45,419: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:25:45,428: INFO: 1977102676: template.py summary success:]\n",
      "[2024-05-14 14:25:45,429: INFO: 1977102676: Starting summary of config\\config.yaml:]\n",
      "[2024-05-14 14:25:45,430: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:25:45,438: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:25:45,676: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:25:45,676: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 5.000000 seconds:]\n",
      "[2024-05-14 14:25:51,506: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:25:51,529: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:25:51,915: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:25:51,935: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:25:51,943: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:25:51,943: INFO: 1977102676: Initial_file: aritfacts_root: aritfacts\n",
      "\n",
      "\n",
      "data_ingestion:\n",
      "  root_dir: artifacts/data_ingestion\n",
      "  local_data_file: Data\n",
      "  load_dir: artifacts/data_ingestion/vector_data.json\n",
      "\n",
      "data_validation:\n",
      "  root_dir: artifacts/data_validation\n",
      "  read_data_dir: artifacts/data_ingestion/vector_data.json\n",
      "  STATUS_FILE: artifacts/data_validation/status.txt\n",
      "\n",
      "data_load:\n",
      "  root_dir: artifacts/data_upload\n",
      "  read_data_dir: artifacts/data_ingestion/vector_data.json\n",
      "  STATUS_FILE: artifacts/data_upload/status.txt\n",
      "\n",
      "code_structure:\n",
      "  root_dir: artifacts/app_schema\n",
      "  load_struct_dir: artifacts/app_schema/schema.json\n",
      "  load_ignored_dir: artifacts/app_schema/ignored_files.json\n",
      "  gitignore_path: .gitignore\n",
      "  code_dir: .\n",
      "  sructure_file: structure.txt\n",
      "\n",
      "json_summary:\n",
      "  root_dir: artifacts/json_summary\n",
      "  read_schema: artifacts/app_schema/schema.json\n",
      "  load_json_summary: artifacts/json_summary/json_summary.json\n",
      "\n",
      "graph_structure:\n",
      "  root_dir: artifacts/graph_structure\n",
      "  graph_structure_file: artifacts/graph_structure/graph_model\n",
      "  graph_json_model: artifacts/graph_structure/graph_json_model.json\n",
      "  sructure_file: structure.txt\n",
      "\n",
      "\n",
      " \n",
      ":]\n",
      "[2024-05-14 14:25:51,943: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:25:51,943: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:25:51,943: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:25:51,960: INFO: 1977102676: config.yaml summary success:]\n",
      "[2024-05-14 14:25:51,961: INFO: 1977102676: Starting summary of src\\vector_db_pipeline\\__init__.py:]\n",
      "[2024-05-14 14:25:51,962: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:25:51,989: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:25:52,207: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:25:52,207: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 3.000000 seconds:]\n",
      "[2024-05-14 14:25:57,480: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:25:57,508: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:25:59,193: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:25:59,213: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:25:59,226: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:25:59,226: INFO: 1977102676: Initial_file: import os \n",
      "import sys\n",
      "import logging\n",
      "\n",
      "logging_str = \"[%(asctime)s: %(levelname)s: %(module)s: %(message)s:]\"\n",
      "\n",
      "log_dir = \"logs\"\n",
      "log_filepath = os.path.join(log_dir,\"running_logs.log\")\n",
      "os.makedirs(log_dir,exist_ok=True)\n",
      "\n",
      "logging.basicConfig(\n",
      "    level = logging.INFO,\n",
      "    format=logging_str,\n",
      "\n",
      "    handlers=[\n",
      "        logging.FileHandler(log_filepath),\n",
      "        logging.StreamHandler(sys.stdout)\n",
      "    ]\n",
      ")\n",
      "\n",
      "logger = logging.getLogger(\"vector_db_Logger\") \n",
      ":]\n",
      "[2024-05-14 14:25:59,226: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:25:59,226: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:25:59,226: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:25:59,226: INFO: 1977102676: __init__.py summary success:]\n",
      "[2024-05-14 14:25:59,226: INFO: 1977102676: Starting summary of src\\vector_db_pipeline\\__init___summary.txt:]\n",
      "[2024-05-14 14:25:59,243: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:25:59,273: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:25:59,483: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:25:59,483: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 1.000000 seconds:]\n",
      "[2024-05-14 14:26:01,480: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:26:01,496: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:26:01,719: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:26:01,721: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 1.000000 seconds:]\n",
      "[2024-05-14 14:26:03,050: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:26:03,066: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:26:03,082: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:26:03,082: INFO: 1977102676: Initial_file: **Summary:** This code sets up a logging system for a Python application, configuring the logging level, format, and handlers for file and console output.\n",
      "\n",
      "**Features:**\n",
      "\n",
      "* Importing necessary modules (os, sys, logging)\n",
      "* Defining a logging string format\n",
      "* Creating a log directory and file\n",
      "* Configuring logging basic configuration\n",
      "* Setting up logging handlers for file and console output\n",
      "* Creating a logger instance \n",
      ":]\n",
      "[2024-05-14 14:26:03,082: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:26:03,082: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:26:03,082: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:26:03,098: INFO: 1977102676: __init___summary.txt summary success:]\n",
      "[2024-05-14 14:26:03,098: INFO: 1977102676: Starting summary of src\\vector_db_pipeline\\components\\data_ingestion.py:]\n",
      "[2024-05-14 14:26:03,109: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:26:03,114: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:26:03,371: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:26:03,371: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 11.000000 seconds:]\n",
      "[2024-05-14 14:26:17,068: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:26:17,105: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:26:18,541: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:26:18,557: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:26:18,583: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:26:18,584: INFO: 1977102676: Initial_file: import pandas as pd\n",
      "from langchain.embeddings.openai import OpenAIEmbeddings\n",
      "from langchain.text_splitter import CharacterTextSplitter\n",
      "from pathlib import Path\n",
      "from typing import List\n",
      "from vector_db_pipeline.entity.config_entity import DataIngestionConfig\n",
      "from vector_db_pipeline import logger\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\"\"\"\n",
      "Processes text data by splitting it into chunks and embedding each chunk.\n",
      "\n",
      "Attributes:\n",
      "    config (DataIngestionConfig): Configuration object containing text splitting settings.\n",
      "\n",
      "Methods:\n",
      "    get_text_chunks(text: str) -> List[str]: Splits input text into chunks based on configuration settings.\n",
      "    split_text(data: List[dict]) -> List[dict]: Splits text data in each dictionary entry into chunks and embeds each chunk.\n",
      "    load_data_json(splited_text_data: List[dict]): Saves the processed data into a JSON file.\n",
      "\"\"\"\n",
      "class TextProcessor:\n",
      "    def __init__(self, config: DataIngestionConfig):\n",
      "        \"\"\"\n",
      "        Initializes TextProcessor with the provided data ingestion configuration.\n",
      "\n",
      "        Args:\n",
      "            config (DataIngestionConfig): Configuration object containing text splitting settings.\n",
      "        \"\"\"\n",
      "        self.config = config\n",
      "        \n",
      "    def get_text_chunks(self, text: str) -> List[str]:\n",
      "        \"\"\"\n",
      "        Splits input text into chunks based on configuration settings.\n",
      "\n",
      "        Args:\n",
      "            text (str): Input text to be split into chunks.\n",
      "\n",
      "        Returns:\n",
      "            chunks (List[str]): List of text chunks.\n",
      "        \"\"\"\n",
      "        text_splitter_config = self.config.text_spliter_config\n",
      "        text_splitter = CharacterTextSplitter(\n",
      "            separator=text_splitter_config.SEPARATOR.encode().decode('unicode_escape'),\n",
      "            chunk_size=text_splitter_config.CHUNK_SIZE,\n",
      "            chunk_overlap=text_splitter_config.CHUNK_OVERLAP,\n",
      "            length_function=len\n",
      "        )\n",
      "        chunks = text_splitter.split_text(text)\n",
      "        return chunks\n",
      "\n",
      "    def split_text(self, data: List[dict]) -> List[dict]:\n",
      "        \"\"\"\n",
      "        Splits text data in each dictionary entry into chunks and embeds each chunk.\n",
      "\n",
      "        Args:\n",
      "            data (List[dict]): List of dictionaries containing text data.\n",
      "\n",
      "        Returns:\n",
      "            splited_text_data (List[dict]): List of dictionaries containing split and embedded text data.\n",
      "        \"\"\"\n",
      "        embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
      "        splited_text_data = []\n",
      "        idx = 0\n",
      "        for d in data:\n",
      "            # Start schema extraction\n",
      "            text = d.get('text')\n",
      "            if text:\n",
      "                timestamp = d.pop('date_scraped_timestamp')\n",
      "                host = d.pop('host')\n",
      "                url = d.pop('url')\n",
      "                page_title = d.pop('page_title')\n",
      "                # End schema extraction\n",
      "                \n",
      "                text_chunks = self.get_text_chunks(text)\n",
      "                \n",
      "                embeded_text = embed_model.embed_documents(text_chunks)\n",
      "                for i, text_chunk in enumerate(text_chunks):\n",
      "                    emb_vect = {'id': str(timestamp)+'-'+str(idx), 'values': embeded_text[i], \n",
      "                                'text': text_chunk, 'host': str(host), 'page_title': str(page_title),\n",
      "                                'url': str(url)}\n",
      "                    idx += 1\n",
      "                    splited_text_data.append(emb_vect)\n",
      "        logger.info(f\"Text processed and chunked. Total chunks: {len(splited_text_data)}\")\n",
      "        return splited_text_data\n",
      "    \n",
      "    def load_data_json(self, splited_text_data: List[dict]):\n",
      "        \"\"\"\n",
      "        Saves the processed data into a JSON file.\n",
      "\n",
      "        Args:\n",
      "            splited_text_data (List[dict]): List of dictionaries containing processed text data.\n",
      "        \"\"\"\n",
      "        json_file_path = Path(self.config.load_dir)\n",
      "        df = pd.DataFrame(splited_text_data)\n",
      "        df.to_json(json_file_path, orient='records')\n",
      "\n",
      "        logger.info(f\"Data processed and saved into JSON file in {json_file_path}\")\n",
      " \n",
      ":]\n",
      "[2024-05-14 14:26:18,586: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:26:18,587: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:26:18,588: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:26:18,591: INFO: 1977102676: data_ingestion.py summary success:]\n",
      "[2024-05-14 14:26:18,591: INFO: 1977102676: Starting summary of src\\vector_db_pipeline\\components\\data_ingestion_summary.txt:]\n",
      "[2024-05-14 14:26:18,607: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:26:18,624: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:26:18,940: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:26:18,940: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 3.000000 seconds:]\n",
      "[2024-05-14 14:26:25,631: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:26:25,652: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:26:27,426: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:26:27,462: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:26:27,475: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:26:27,476: INFO: 1977102676: Initial_file: **Summary:**\n",
      "The provided code defines a `TextProcessor` class that processes text data by splitting it into chunks and embedding each chunk using OpenAI embeddings. The class takes a `DataIngestionConfig` object as input and provides three methods: `get_text_chunks` to split text into chunks, `split_text` to split and embed text data, and `load_data_json` to save the processed data to a JSON file.\n",
      "\n",
      "**Features:**\n",
      "\n",
      "* `TextProcessor` class\n",
      "* `get_text_chunks` method to split text into chunks\n",
      "* `split_text` method to split and embed text data\n",
      "* `load_data_json` method to save processed data to a JSON file\n",
      "* Uses OpenAI embeddings for text embedding\n",
      "* Configurable text splitting settings through `DataIngestionConfig` object \n",
      ":]\n",
      "[2024-05-14 14:26:27,476: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:26:27,477: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:26:27,478: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:26:27,481: INFO: 1977102676: data_ingestion_summary.txt summary success:]\n",
      "[2024-05-14 14:26:27,481: INFO: 1977102676: Starting summary of src\\vector_db_pipeline\\components\\data_load.py:]\n",
      "[2024-05-14 14:26:27,491: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:26:27,514: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:26:27,739: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:26:27,754: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 14.000000 seconds:]\n",
      "[2024-05-14 14:26:43,054: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:26:43,085: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:26:43,472: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:26:43,488: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:26:43,503: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:26:43,503: INFO: 1977102676: Initial_file: from vector_db_pipeline.entity.config_entity import DataUploadConfig\n",
      "from vector_db_pipeline import logger\n",
      "from pinecone import Pinecone, PodSpec\n",
      "import math\n",
      "import time\n",
      "import os\n",
      "from dotenv import load_dotenv\n",
      "import pandas as pd\n",
      "\n",
      "\"\"\"\n",
      "Handles data upload to Pinecone indexes.\n",
      "\n",
      "Attributes:\n",
      "    config (DataUploadConfig): Configuration object containing settings for data upload.\n",
      "    params (dict): Dictionary containing parameters required for data upload.\n",
      "\n",
      "Methods:\n",
      "    del_index(): Deletes the specified index if it exists.\n",
      "    recreate_index(): Recreates the index with specified dimensions, metric, and environment.\n",
      "    pinecon_vector(): Converts data from CSV to a list of JSON objects.\n",
      "    batch_upload(pinecone_vector): Uploads vectors to a Pinecone index in batches.\n",
      "\"\"\"\n",
      "class DataUpload:\n",
      "    def __init__(self, config: DataUploadConfig):\n",
      "        \"\"\"\n",
      "        Initializes DataUpload class with provided configuration and parameters.\n",
      "\n",
      "        Args:\n",
      "            config (DataUploadConfig): Configuration object containing settings for data upload.\n",
      "            params_filepath (str): Filepath to parameters file. Defaults to PARAMS_FILE_PATH.\n",
      "        \"\"\"\n",
      "\n",
      "\n",
      "        self.config = config\n",
      "\n",
      "        #initialize db\n",
      "        load_dotenv()\n",
      "        pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
      "        self.pc = Pinecone(api_key=pinecone_api_key)\n",
      "        self.index_info = self.config.index_info\n",
      "        self.index_name = self.index_info.INDEX_NAME\n",
      "        \n",
      "        \n",
      "     \n",
      "    def del_index(self):\n",
      "        \"\"\"\n",
      "        Deletes the specified index if it exists.\n",
      "        \"\"\"\n",
      "        if self.index_name in [index_info[\"name\"] for index_info in self.pc.list_indexes()]:\n",
      "            self.pc.delete_index(self.index_name)\n",
      "            logger.info(f\"Index '{self.index_name}' deleted \")\n",
      "\n",
      "    def recreate_index(self):\n",
      "        \"\"\"\n",
      "        Recreates the index with specified dimensions, metric, and environment.\n",
      "        \"\"\"\n",
      "\n",
      "        dim = self.index_info.DIMENSIONS\n",
      "        met = self.index_info.METRIC\n",
      "        env = self.index_info.ENVIROMENT\n",
      "        existing_indexes = [index_info[\"name\"] for index_info in self.pc.list_indexes()]\n",
      "        \n",
      "        # Check if index already exists\n",
      "        if self.index_name not in existing_indexes:\n",
      "            # Create index if it doesn't exist\n",
      "            self.pc.create_index(\n",
      "                name=self.index_name,\n",
      "                dimension=dim,\n",
      "                metric=met,\n",
      "                spec=PodSpec(\n",
      "                    environment=env\n",
      "                )\n",
      "            )\n",
      "            # Wait for index to be initialized\n",
      "            while not self.pc.describe_index(self.index_name).status['ready']:\n",
      "                time.sleep(1)\n",
      "        index = self.pc.Index(self.index_name)\n",
      "        logger.info(\"Index created\")\n",
      "        logger.info(index.describe_index_stats())\n",
      "\n",
      "    def pinecon_vector(self): \n",
      "        \"\"\"\n",
      "        Converts data  to a list of JSON objects.\n",
      "\n",
      "        Returns:\n",
      "            pinecone_vect (list): List of JSON objects representing each row of the dataframe.\n",
      "        \"\"\"\n",
      "        data_read_path = self.config.read_data_dir\n",
      "        df = pd.read_json(data_read_path, orient='records')\n",
      "        pinecone_vect = []\n",
      "        \n",
      "        for i, row in df.iterrows():\n",
      "            id = row['id']\n",
      "            vectors = row['values']\n",
      "            text = row['text']\n",
      "            host = row['host']\n",
      "            page_title = row['page_title']\n",
      "            url = row['url']\n",
      "            # Create a dictionary for the metadata containing 'text', 'host', 'page_title', and 'url'\n",
      "            metadata = {'text': text, 'host': host, 'page_title': page_title, 'url': url}\n",
      "            # Create a dictionary for the JSON object containing 'id', 'values', and 'metadata'\n",
      "            emb_vect = {'id': id, 'values': vectors, 'metadata': metadata}\n",
      "            \n",
      "            pinecone_vect.append(emb_vect)\n",
      "        logger.info(f\"Data ready for upload\")\n",
      "        with open(self.config.STATUS_FILE, 'a') as f:\n",
      "            f.write(f\"Data size: {len(pinecone_vect)}\\n\")\n",
      "        # Return the list of JSON objects\n",
      "        return pinecone_vect\n",
      "\n",
      "    def batch_upload(self, pinecone_vector):\n",
      "        \"\"\"\n",
      "        Uploads vectors to a Pinecone index in batches.\n",
      "\n",
      "        Args:\n",
      "            pinecone_vector (list): List of JSON objects representing vectors to be uploaded.\n",
      "        \"\"\"\n",
      "        # Determine the batch size and total number of data points\n",
      "        batch_size = self.config.batch_size.BATCH_SIZE \n",
      "        namespace = self.index_info.NAMESPACE\n",
      "        index = self.pc.Index(self.index_name)\n",
      "        data_size = len(pinecone_vector)\n",
      "        \n",
      "        \n",
      "        # Calculate the number of batches required\n",
      "        batch_num = math.ceil(data_size / batch_size)\n",
      "        logger.info(f\"Uploading: {data_size} vectors, in {batch_num} batches\")\n",
      "        \n",
      "        # Iterate over each batch\n",
      "        for i in range(batch_num):\n",
      " \n",
      "            try:\n",
      "                # Calculate the start and end indices for the current batch\n",
      "                start_idx = i * batch_size\n",
      "                end_idx = min((i + 1) * batch_size, len(pinecone_vector))\n",
      "            \n",
      "                batch_vectors = pinecone_vector[start_idx:end_idx]\n",
      "                \n",
      "                # Upload the vectors to the Pinecone index\n",
      "                index.upsert(vectors=batch_vectors,namespace=namespace)\n",
      "                logger.info(f\"Batch {i+1} uploaded\")\n",
      "            except Exception as e:\n",
      "                logger.info(f\"Error encountered: {e}\")\n",
      "        \n",
      "        time.sleep(30)\n",
      "        logger.info(index.describe_index_stats())\n",
      "        with open(self.config.STATUS_FILE, 'a') as f:\n",
      "            f.write(f\"Data upload completed\\n\")\n",
      " \n",
      ":]\n",
      "[2024-05-14 14:26:43,503: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:26:43,503: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:26:43,503: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:26:43,519: INFO: 1977102676: data_load.py summary success:]\n",
      "[2024-05-14 14:26:43,519: INFO: 1977102676: Starting summary of src\\vector_db_pipeline\\components\\data_load_summary.txt:]\n",
      "[2024-05-14 14:26:43,529: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:26:43,551: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:26:43,771: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:26:43,771: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 4.000000 seconds:]\n",
      "[2024-05-14 14:26:49,597: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:26:49,614: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:26:49,848: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:26:49,849: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 1.000000 seconds:]\n",
      "[2024-05-14 14:26:51,417: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:26:51,449: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:26:51,449: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:26:51,449: INFO: 1977102676: Initial_file: **Summary:**\n",
      "This is a Python class named `DataUpload` that handles data upload to Pinecone indexes. It takes a `DataUploadConfig` object as an input and provides methods to delete an index, recreate an index, convert data to a list of JSON objects, and upload vectors to a Pinecone index in batches.\n",
      "\n",
      "**Functions:**\n",
      "\n",
      "1. `__init__`: Initializes the `DataUpload` class with a `DataUploadConfig` object.\n",
      "2. `del_index`: Deletes a specified index if it exists.\n",
      "3. `recreate_index`: Recreates an index with specified dimensions, metric, and environment.\n",
      "4. `pinecon_vector`: Converts data to a list of JSON objects.\n",
      "5. `batch_upload`: Uploads vectors to a Pinecone index in batches.\n",
      "\n",
      "**Classes:**\n",
      "\n",
      "1. `DataUpload`: The main class that handles data upload to Pinecone indexes.\n",
      "\n",
      "**Methods:**\n",
      "\n",
      "1. `del_index`\n",
      "2. `recreate_index`\n",
      "3. `pinecon_vector`\n",
      "4. `batch_upload`\n",
      "\n",
      "**Dependencies:**\n",
      "\n",
      "1. `vector_db_pipeline.entity.config_entity`: For `DataUploadConfig` class.\n",
      "2. `pinecone`: For Pinecone API interactions.\n",
      "3. `dotenv`: For loading environment variables.\n",
      "4. `pandas`: For data manipulation.\n",
      "5. `math`: For mathematical operations.\n",
      "6. `time`: For timing and waiting.\n",
      "7. `os`: For environment variable access. \n",
      ":]\n",
      "[2024-05-14 14:26:51,449: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:26:51,465: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:26:51,465: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:26:51,465: INFO: 1977102676: data_load_summary.txt summary success:]\n",
      "[2024-05-14 14:26:51,465: INFO: 1977102676: Starting summary of src\\vector_db_pipeline\\components\\data_validation.py:]\n",
      "[2024-05-14 14:26:51,479: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:26:51,496: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:26:51,734: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:26:51,734: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 13.000000 seconds:]\n",
      "[2024-05-14 14:27:07,341: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:27:07,357: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:27:08,242: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:27:08,276: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:27:08,287: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:27:08,288: INFO: 1977102676: Initial_file: from vector_db_pipeline.entity.config_entity import DataValidationConfig\n",
      "from vector_db_pipeline import logger\n",
      "import pandas as pd\n",
      "\n",
      "\n",
      "\"\"\"\n",
      "Validates data integrity and structure based on specified schema and configuration settings.\n",
      "\n",
      "Attributes:\n",
      "    config (DataValidationConfig): Configuration object containing data validation settings.\n",
      "\n",
      "Methods:\n",
      "    validate_all_columns() -> bool: Validates presence of all columns specified in the schema.\n",
      "    validate_unique_index() -> bool: Validates uniqueness of the 'id' column as the index.\n",
      "\"\"\"\n",
      "class DataValidation:\n",
      "    def __init__(self, config: DataValidationConfig):\n",
      "        \"\"\"\n",
      "        Initializes DataValidation with the provided data validation configuration.\n",
      "\n",
      "        Args:\n",
      "            config (DataValidationConfig): Configuration object containing data validation settings.\n",
      "        \"\"\"\n",
      "        self.config = config\n",
      "        self.data = pd.read_json(self.config.read_data_dir, orient='records')\n",
      "\n",
      "    def validate_all_columns(self) -> bool:\n",
      "        \"\"\"\n",
      "        Validates presence of all columns specified in the schema.\n",
      "\n",
      "        Returns:\n",
      "            bool: True if all columns are present, False otherwise.\n",
      "        \"\"\"\n",
      "        try:\n",
      "            all_cols = list(self.data.columns)\n",
      "            all_schema = self.config.SCHEMA\n",
      "            all_schema['id'] = 'str'\n",
      "            all_schema['values'] = 'list'\n",
      "\n",
      "            validation_status = all(col in all_schema.keys() for col in all_cols)\n",
      "\n",
      "            with open(self.config.STATUS_FILE, 'a') as f:\n",
      "                f.write(f\"All columns present in data: {validation_status}\\n\")\n",
      "\n",
      "            logger.info(f\"All columns present in data: {validation_status}\")\n",
      "            return validation_status\n",
      "        \n",
      "        except Exception as e:\n",
      "            logger.error(f\"Error occurred during column validation: {str(e)}\")\n",
      "            raise e\n",
      "        \n",
      "    def validate_unique_index(self) -> bool:\n",
      "        \"\"\"\n",
      "        Validates uniqueness of the 'id' column as the index.\n",
      "\n",
      "        Returns:\n",
      "            bool: True if 'id' column is unique, False otherwise.\n",
      "        \"\"\"\n",
      "        try:\n",
      "            unique_id = len(self.data.id.unique()) == len(self.data)\n",
      "\n",
      "            with open(self.config.STATUS_FILE, 'a') as f:\n",
      "                f.write(f\"Unique ids: {unique_id}\\n\")\n",
      "\n",
      "            logger.info(f\"Unique ids: {unique_id}\")\n",
      "            return unique_id\n",
      "        \n",
      "        except Exception as e:\n",
      "            logger.error(f\"Error occurred during unique index validation: {str(e)}\")\n",
      "            raise e\n",
      "        \n",
      "    def validate_column_types(self):\n",
      "        \"\"\"\n",
      "        Get the types of values in each column of the DataFrame.\n",
      "\n",
      "       \n",
      "        Returns:\n",
      "            dict: A dictionary where keys are column names and values are lists containing the types of values present in each column.\n",
      "        \"\"\"\n",
      "        \n",
      "        try:\n",
      "            column_types = {}\n",
      "            how_many_types = {}\n",
      "            \n",
      "            # Iterate over each column in the DataFrame\n",
      "            for column in self.data.columns:\n",
      "                # Create a set of unique types for values in the column\n",
      "                types = set(type(value) for value in self.data[column])\n",
      "                # Convert the set to a list for easier handling\n",
      "                types_list = list(types)\n",
      "\n",
      "                column_types[column] = types_list\n",
      "                how_many_types[column] = len(types_list)\n",
      "            \n",
      "            \n",
      "\n",
      "            # Check if elements in 'values' column are of type float\n",
      "            vector_elemts_type = list(set(type(elem) for elem in self.data['values'][0]))\n",
      "            if len(vector_elemts_type) != 1 or vector_elemts_type[0] != float:\n",
      "                raise TypeError(\"Vector values are not of type float\")\n",
      "\n",
      "            # Check if all columns have a single type\n",
      "            unique_types = all(value == 1 for value in how_many_types.values())\n",
      "\n",
      "            # Check if 'id' column is of type string and 'values' column is of type list\n",
      "            id_type = column_types['id'][0] == str\n",
      "            vector_type = column_types['values'][0] == list\n",
      "\n",
      "            if not id_type:\n",
      "                raise TypeError(\"ID column is not of type string\")\n",
      "            elif not vector_type:\n",
      "                raise TypeError(\"Values column is not of type list\")\n",
      "            elif not unique_types:\n",
      "                raise TypeError(\"Several types present in data\")\n",
      "\n",
      "            # Log success message\n",
      "            with open(self.config.STATUS_FILE, 'a') as f:\n",
      "                f.write(\"Data types are correct\\n\")\n",
      "\n",
      "            return logger.info(\"Data types are valid\")\n",
      "        except Exception as e:\n",
      "            raise e\n",
      "\n",
      " \n",
      ":]\n",
      "[2024-05-14 14:27:08,289: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:27:08,291: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:27:08,291: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:27:08,291: INFO: 1977102676: data_validation.py summary success:]\n",
      "[2024-05-14 14:27:08,291: INFO: 1977102676: Starting summary of src\\vector_db_pipeline\\components\\data_validation_summary.txt:]\n",
      "[2024-05-14 14:27:08,307: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:27:08,347: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:27:08,577: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:27:08,580: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 2.000000 seconds:]\n",
      "[2024-05-14 14:27:12,308: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:27:12,341: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:27:13,107: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:27:13,124: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:27:13,124: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:27:13,124: INFO: 1977102676: Initial_file: **Summary:**\n",
      "This is a Python class named `DataValidation` that validates the integrity and structure of data based on a specified schema and configuration settings. It checks for the presence of all columns, uniqueness of the 'id' column, and correct data types in each column.\n",
      "\n",
      "**Functions/Methods:**\n",
      "\n",
      "1. `__init__`: Initializes the `DataValidation` object with a `DataValidationConfig` object.\n",
      "2. `validate_all_columns`: Validates the presence of all columns specified in the schema.\n",
      "3. `validate_unique_index`: Validates the uniqueness of the 'id' column as the index.\n",
      "4. `validate_column_types`: Validates the data types of each column in the DataFrame.\n",
      "\n",
      "**Classes:**\n",
      "\n",
      "1. `DataValidation`: The main class that performs data validation.\n",
      "\n",
      "**Dependencies:**\n",
      "\n",
      "1. `vector_db_pipeline.entity.config_entity`: Imported for `DataValidationConfig`.\n",
      "2. `vector_db_pipeline`: Imported for `logger`.\n",
      "3. `pandas`: Imported as `pd` for data manipulation.\n",
      "4. `DataValidationConfig`: A configuration object containing data validation settings. \n",
      ":]\n",
      "[2024-05-14 14:27:13,141: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:27:13,141: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:27:13,141: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:27:13,141: INFO: 1977102676: data_validation_summary.txt summary success:]\n",
      "[2024-05-14 14:27:13,141: INFO: 1977102676: Starting summary of src\\vector_db_pipeline\\components\\generate_app_structure.py:]\n",
      "[2024-05-14 14:27:13,188: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:27:13,207: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:27:13,442: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:27:13,442: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 18.000000 seconds:]\n",
      "[2024-05-14 14:27:36,227: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:27:36,241: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:27:38,242: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:27:38,259: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:27:38,275: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:27:38,275: INFO: 1977102676: Initial_file: from vector_db_pipeline.entity.config_entity import CodeStructureConfig\n",
      "from vector_db_pipeline.constants import *\n",
      "from vector_db_pipeline.utils.common import  save_json, set_to_txt\n",
      "from vector_db_pipeline import logger\n",
      "from dotenv import load_dotenv\n",
      "from langchain_core.prompts import ChatPromptTemplate\n",
      "from langchain_groq import ChatGroq\n",
      "load_dotenv()\n",
      "\n",
      "import os\n",
      "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
      "os.environ[\"LANGCHAIN_PROJECT\"] = \"Code_structure_main\"\n",
      "\n",
      "\n",
      "\"\"\"\n",
      "A class for managing code structure and formatting.\n",
      "\n",
      "Attributes:\n",
      "    config (CodeStructureConfig): Configuration object for the CodeStructure class.\n",
      "\n",
      "Functions:\n",
      "    get_ignored_subdirs_from_gitignore: Reads ignored directories and extensions from a .gitignore file.\n",
      "    explore_directory: Explores directories and files, excluding ignored ones.\n",
      "    build_directory_structure: Builds the directory structure recursively.\n",
      "    get_formated_strcuture: Formats the directory structure using an AI model.\n",
      "\"\"\"\n",
      "\n",
      "class CodeStructure:\n",
      "    def __init__(self, config:CodeStructureConfig):\n",
      "        \"\"\"\n",
      "        Initializes the CodeStructure object with the given configuration.\n",
      "\n",
      "        Args:\n",
      "            config (CodeStructureConfig): Configuration object for the CodeStructure class.\n",
      "        \"\"\"        \n",
      "        self.config = config        \n",
      "\n",
      "    def get_ignored_subdirs_from_gitignore(self):\n",
      "        \"\"\"\n",
      "        Reads ignored directories and extensions from a .gitignore file.\n",
      "\n",
      "        Returns:\n",
      "            None\n",
      "        \"\"\"\n",
      "        ignore_subdirs_files = []\n",
      "        ignore_subdirs_files_extentions = []\n",
      "        gitignore_path = self.config.gitignore_path\n",
      "        ignored_fles_path = self.config.load_ignored_dir\n",
      "        try:\n",
      "            with open(gitignore_path, \"r\") as file:\n",
      "                for i,line in enumerate(file):\n",
      "                    # Skip comments and empty lines\n",
      "                    line = line.strip()\n",
      "                    if not line or line.startswith(\"#\"):\n",
      "                        continue\n",
      "                    else:\n",
      "                        if line.startswith(\"*\"):\n",
      "                            if line.endswith(\"/\"):\n",
      "                                ignore_subdirs_files_extentions.append(line[1:-1])\n",
      "                            else:\n",
      "                                ignore_subdirs_files_extentions.append(line[1:])\n",
      "                        else:\n",
      "                            if line.endswith(\"/\"):\n",
      "                                ignore_subdirs_files.append(line[:-1])\n",
      "                            elif line.endswith(\"*\"):\n",
      "                                ignore_subdirs_files.append(line[:-2])\n",
      "                            \n",
      "                            else:\n",
      "                                ignore_subdirs_files.append(line)\n",
      "            logger.info(f\"Ignored files obtained from: {gitignore_path}\")\n",
      "        except FileNotFoundError:\n",
      "            return logger.info(f\"Warning: {gitignore_path} not found.\")\n",
      "        except Exception as e:\n",
      "            return logger.info(f\"Error while reading {gitignore_path}: {e}\")\n",
      "\n",
      "        self.ignored_subdirs = set(ignore_subdirs_files)\n",
      "        self.ignored_subdirs.update({'.git', '.github'})\n",
      "        self.ignored_extensions = set(ignore_subdirs_files_extentions)\n",
      "        try:\n",
      "            all_ignored_files = self.ignored_subdirs.union(self.ignored_extensions)\n",
      "            set_to_txt(self.config.load_ignored_dir,all_ignored_files)\n",
      "            return logger.info(f\"Files to ignore in code structure loaded to  : {ignored_fles_path}\")\n",
      "        except Exception as e:\n",
      "            return logger.info(f\"Error while loading ignored files to {ignored_fles_path}: {e}\")\n",
      "        \n",
      "\n",
      "        \n",
      "    \n",
      "    def explore_directory(self,directory):\n",
      "        \"\"\"\n",
      "        Explores directories and files, excluding ignored ones.\n",
      "\n",
      "        Args:\n",
      "            directory (str): Path to the directory to explore.\n",
      "\n",
      "        Returns:\n",
      "            dict: A dictionary containing the list of directories and files.\n",
      "        \"\"\"\n",
      "        directories = []\n",
      "        files = []\n",
      "        for item in os.listdir(directory):\n",
      "            if item not in self.ignored_subdirs and not item.endswith(tuple(self.ignored_extensions)):\n",
      "                item_path = os.path.join(directory, item)\n",
      "\n",
      "                if os.path.isdir(item_path):\n",
      "                    directories.append(item)\n",
      "                else:\n",
      "                    files.append(item)\n",
      "\n",
      "        return {'Directories': directories, 'Files': files}\n",
      "\n",
      "    def build_directory_structure(self):\n",
      "        \"\"\"\n",
      "        Builds the directory structure recursively.\n",
      "\n",
      "        Returns:\n",
      "            dict: A dictionary representing the directory structure.\n",
      "        \"\"\"\n",
      "        directory_structure = {}\n",
      "        self.root_directory = self.config.code_dir\n",
      "        dir_structure_file = self.config.load_struct_dir\n",
      "        def explore_and_build(directory):\n",
      "            dir_path = os.path.join(self.root_directory, directory)\n",
      "            directory_structure[directory] = self.explore_directory(dir_path)\n",
      "            \n",
      "            for subdir in directory_structure[directory]['Directories']:\n",
      "                explore_and_build(os.path.join(directory, subdir))\n",
      "        \n",
      "        explore_and_build(self.root_directory)\n",
      "        \n",
      "        \n",
      "        save_json(Path(dir_structure_file), directory_structure)\n",
      "        logger.info(f\"Directory structure loaded to {dir_structure_file}\")\n",
      "        return  directory_structure\n",
      "\n",
      "    def get_formated_strcuture(self, directory_structure):\n",
      "        \"\"\"\n",
      "        Formats the directory structure using an AI model.\n",
      "\n",
      "        Args:\n",
      "            directory_structure (dict): The directory structure to format.\n",
      "\n",
      "        Returns:\n",
      "            None\n",
      "        \"\"\"\n",
      "        try:\n",
      "            formated_structure_file = self.config.sructure_file\n",
      "            model = self.config.models.Llama3\n",
      "            logger.info(f\"Working with model: {model}\")\n",
      "            chat = ChatGroq(temperature=0, model_name=model)\n",
      "            file_structure_prompt = self.config.structure_prompt\n",
      "            prompt = ChatPromptTemplate.from_messages([(\"human\", file_structure_prompt)])\n",
      "            chain = prompt | chat\n",
      "            fromated_structure = chain.invoke({\"JSON_FILE\": directory_structure})\n",
      "            with open(formated_structure_file, \"w\") as f:\n",
      "                f.write(fromated_structure.content)\n",
      "            return logger.info(f\"Formated file structure loaded to : {formated_structure_file}\")\n",
      "        except Exception as e:\n",
      "            return logger.info(f\"Error while formatting structure: {e}\")\n",
      "\n",
      " \n",
      ":]\n",
      "[2024-05-14 14:27:38,275: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:27:38,275: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:27:38,275: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:27:38,275: INFO: 1977102676: generate_app_structure.py summary success:]\n",
      "[2024-05-14 14:27:38,275: INFO: 1977102676: Starting summary of src\\vector_db_pipeline\\components\\generate_app_structure_summary.txt:]\n",
      "[2024-05-14 14:27:38,294: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:27:38,309: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:27:40,209: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:27:40,228: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:27:40,581: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:27:40,603: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:27:40,619: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:27:40,619: INFO: 1977102676: Initial_file: **Summary:**\n",
      "This is a Python class `CodeStructure` that manages code structure and formatting. It reads ignored directories and extensions from a `.gitignore` file, explores directories and files, builds the directory structure recursively, and formats the directory structure using an AI model.\n",
      "\n",
      "**Functions:**\n",
      "\n",
      "1. `__init__`: Initializes the `CodeStructure` object with a configuration object.\n",
      "2. `get_ignored_subdirs_from_gitignore`: Reads ignored directories and extensions from a `.gitignore` file.\n",
      "3. `explore_directory`: Explores directories and files, excluding ignored ones.\n",
      "4. `build_directory_structure`: Builds the directory structure recursively.\n",
      "5. `get_formated_strcuture`: Formats the directory structure using an AI model.\n",
      "\n",
      "**Classes:**\n",
      "\n",
      "1. `CodeStructure`: The main class that manages code structure and formatting.\n",
      "\n",
      "**Methods:**\n",
      "\n",
      "1. `__init__`\n",
      "2. `get_ignored_subdirs_from_gitignore`\n",
      "3. `explore_directory`\n",
      "4. `build_directory_structure`\n",
      "5. `get_formated_strcuture` \n",
      ":]\n",
      "[2024-05-14 14:27:40,620: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:27:40,622: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:27:40,623: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:27:40,635: INFO: 1977102676: generate_app_structure_summary.txt summary success:]\n",
      "[2024-05-14 14:27:40,636: INFO: 1977102676: Starting summary of src\\vector_db_pipeline\\components\\__init__.py:]\n",
      "[2024-05-14 14:27:40,637: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:27:40,639: INFO: 1977102676: generate_app_structure_summary.txt empty.:]\n",
      "[2024-05-14 14:27:40,640: INFO: 1977102676: Starting summary of src\\vector_db_pipeline\\config\\configuration.py:]\n",
      "[2024-05-14 14:27:40,644: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:27:40,675: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:27:40,943: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:27:40,943: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 14.000000 seconds:]\n",
      "[2024-05-14 14:27:58,080: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:27:58,100: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:27:59,078: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:27:59,095: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:27:59,110: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:27:59,110: INFO: 1977102676: Initial_file: from vector_db_pipeline.constants import *\n",
      "from vector_db_pipeline.utils.common import read_yaml, create_directories\n",
      "from vector_db_pipeline.entity.config_entity import (DataIngestionConfig,\n",
      "                                                     DataValidationConfig,\n",
      "                                                     DataUploadConfig,\n",
      "                                                     CodeStructureConfig)\n",
      "\n",
      "\n",
      "\"\"\"\n",
      "Manages configuration settings for data ingestion, validation, upload, and code structure.\n",
      "\n",
      "Attributes:\n",
      "    config (dict): Dictionary containing data ingestion, validation, upload, and code structure configuration settings.\n",
      "    schema (dict): Dictionary containing schema configuration settings.\n",
      "    params (dict): Dictionary containing parameters required for configuration settings.\n",
      "    models (dict): Dictionary containing model configuration settings.\n",
      "    prompt_template (dict): Dictionary containing prompt template configuration settings.\n",
      "\n",
      "Methods:\n",
      "    get_data_ingestion_config(): Retrieves data ingestion configuration settings.\n",
      "    get_data_validation_config(): Retrieves data validation configuration settings.\n",
      "    get_data_upload_config(): Retrieves data upload configuration settings.\n",
      "    get_code_structure_config(): Retrieves code structure configuration settings.\n",
      "\"\"\"\n",
      "class ConfigurationManager:\n",
      "    def __init__( self,\n",
      "        config_filepath = CONFIG_FILE_PATH,\n",
      "        schema_filepath = SCHEMA_FILE_PATH,\n",
      "        params_filepath = PARAMS_FILE_PATH,\n",
      "        models_filepath = MODELS_FILE_PATH,\n",
      "        prompt_template = PROMPT_FILE_PATH):\n",
      "        \"\"\"\n",
      "        Initializes ConfigurationManager with provided filepaths.\n",
      "\n",
      "        Args:\n",
      "            config_filepath (str): Filepath to configuration file. Defaults to CONFIG_FILE_PATH.\n",
      "            schema_filepath (str): Filepath to schema file. Defaults to SCHEMA_FILE_PATH.\n",
      "            params_filepath (str): Filepath to parameters file. Defaults to PARAMS_FILE_PATH.\n",
      "            models_filepath (str): Filepath to models file. Defaults to MODELS_FILE_PATH.\n",
      "            prompt_template (str): Filepath to prompt template file. Defaults to PROMPT_FILE_PATH.\n",
      "        \"\"\"\n",
      "        self.config = read_yaml(config_filepath)\n",
      "        self.schema = read_yaml(schema_filepath)\n",
      "        self.params = read_yaml(params_filepath)\n",
      "        self.models = read_yaml(models_filepath)\n",
      "        self.prompt_template = read_yaml(prompt_template)\n",
      "\n",
      "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
      "        \"\"\"\n",
      "        Retrieves data ingestion configuration settings.\n",
      "\n",
      "        Returns:\n",
      "            data_ingestion_config (DataIngestionConfig): Data ingestion configuration object.\n",
      "        \"\"\"\n",
      "        config = self.config.data_ingestion\n",
      "        text_spliter = self.params.TEXT_SPLITER\n",
      "        namespace = self.params.INDEX_INFO.NAMESPACE\n",
      "\n",
      "        create_directories([config.root_dir])\n",
      "\n",
      "        data_ingestion_config = DataIngestionConfig(\n",
      "            root_dir=config.root_dir,\n",
      "            local_data_file=config.local_data_file,\n",
      "            load_dir=config.load_dir,\n",
      "            text_spliter_config=text_spliter,\n",
      "            namespace_idx = namespace\n",
      "        )\n",
      "\n",
      "        return data_ingestion_config\n",
      "\n",
      "    def get_data_validation_config(self) -> DataValidationConfig:\n",
      "        \"\"\"\n",
      "        Retrieves data validation configuration settings.\n",
      "\n",
      "        Returns:\n",
      "            data_validation_config (DataValidationConfig): Data validation configuration object.\n",
      "        \"\"\"\n",
      "        config = self.config.data_validation\n",
      "        schema = self.schema.COLUMNS\n",
      "\n",
      "        create_directories([config.root_dir])\n",
      "\n",
      "        data_validation_config = DataValidationConfig(\n",
      "            root_dir=config.root_dir,\n",
      "            read_data_dir=config.read_data_dir,\n",
      "            STATUS_FILE=config.STATUS_FILE,\n",
      "            SCHEMA=schema\n",
      "        )\n",
      "\n",
      "        return data_validation_config\n",
      "\n",
      "    def get_data_upload_config(self) -> DataUploadConfig:\n",
      "        \"\"\"\n",
      "        Retrieves data upload configuration settings.\n",
      "\n",
      "        Returns:\n",
      "            data_upload_config (DataUploadConfig): Data upload configuration object.\n",
      "        \"\"\"\n",
      "        config = self.config.data_load\n",
      "        index_info = self.params.INDEX_INFO\n",
      "        batch_size = self.params.BATCH_SIZE\n",
      "\n",
      "        create_directories([config.root_dir])\n",
      "\n",
      "        data_upload_config = DataUploadConfig(\n",
      "            root_dir=config.root_dir,\n",
      "            read_data_dir=config.read_data_dir,\n",
      "            STATUS_FILE=config.STATUS_FILE,\n",
      "            index_info=index_info,\n",
      "            batch_size=batch_size\n",
      "        )\n",
      "\n",
      "        return data_upload_config\n",
      "    \n",
      "    def get_code_structure_config(self) -> CodeStructureConfig:\n",
      "        \"\"\"\n",
      "        Generates a CodeStructureConfig object based on the provided configuration.\n",
      "\n",
      "        Returns:\n",
      "            CodeStructureConfig: The generated configuration object.\n",
      "        \"\"\"\n",
      "        # Get the configuration from the main configuration object\n",
      "        config = self.config.code_structure\n",
      "        \n",
      "        # Generate the file structure prompt template\n",
      "        prompt_teplate = self.prompt_template.generate_file_structure\n",
      "        \n",
      "        # Create necessary directories\n",
      "        create_directories([config.root_dir])\n",
      "        \n",
      "        # Create a CodeStructureConfig object with the specified parameters\n",
      "        code_structure_config = CodeStructureConfig(\n",
      "            root_dir=config.root_dir,\n",
      "            load_struct_dir=config.load_struct_dir,\n",
      "            load_ignored_dir=config.load_ignored_dir,\n",
      "            gitignore_path=config.gitignore_path,\n",
      "            code_dir=config.code_dir,\n",
      "            sructure_file=config.sructure_file,\n",
      "            models=self.models,\n",
      "            structure_prompt=prompt_teplate.description\n",
      "        ) \n",
      "\n",
      "        return code_structure_config\n",
      "\n",
      "\n",
      " \n",
      ":]\n",
      "[2024-05-14 14:27:59,110: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:27:59,110: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:27:59,110: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:27:59,135: INFO: 1977102676: configuration.py summary success:]\n",
      "[2024-05-14 14:27:59,136: INFO: 1977102676: Starting summary of src\\vector_db_pipeline\\config\\configuration_summary.txt:]\n",
      "[2024-05-14 14:27:59,148: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:27:59,177: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:27:59,405: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:27:59,405: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 1.000000 seconds:]\n",
      "[2024-05-14 14:28:01,462: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:28:01,492: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:28:03,612: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:28:03,627: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:28:03,645: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:28:03,645: INFO: 1977102676: Initial_file: **Summary:**\n",
      "The provided content is a Python class named `ConfigurationManager` that manages configuration settings for data ingestion, validation, upload, and code structure. It reads configuration files in YAML format and provides methods to retrieve configuration settings for each stage.\n",
      "\n",
      "**Functions:**\n",
      "\n",
      "* `__init__`: Initializes the `ConfigurationManager` with filepaths to configuration files.\n",
      "* `get_data_ingestion_config`: Retrieves data ingestion configuration settings.\n",
      "* `get_data_validation_config`: Retrieves data validation configuration settings.\n",
      "* `get_data_upload_config`: Retrieves data upload configuration settings.\n",
      "* `get_code_structure_config`: Retrieves code structure configuration settings.\n",
      "\n",
      "**Classes:**\n",
      "\n",
      "* `ConfigurationManager`: Manages configuration settings for data ingestion, validation, upload, and code structure.\n",
      "* `DataIngestionConfig`: Represents data ingestion configuration settings.\n",
      "* `DataValidationConfig`: Represents data validation configuration settings.\n",
      "* `DataUploadConfig`: Represents data upload configuration settings.\n",
      "* `CodeStructureConfig`: Represents code structure configuration settings.\n",
      "\n",
      "**Methods:**\n",
      "\n",
      "* `get_data_ingestion_config`\n",
      "* `get_data_validation_config`\n",
      "* `get_data_upload_config`\n",
      "* `get_code_structure_config` \n",
      ":]\n",
      "[2024-05-14 14:28:03,645: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:28:03,645: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:28:03,645: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:28:03,661: INFO: 1977102676: configuration_summary.txt summary success:]\n",
      "[2024-05-14 14:28:03,665: INFO: 1977102676: Starting summary of src\\vector_db_pipeline\\config\\__init__.py:]\n",
      "[2024-05-14 14:28:03,665: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:28:03,665: INFO: 1977102676: configuration_summary.txt empty.:]\n",
      "[2024-05-14 14:28:03,665: INFO: 1977102676: Starting summary of src\\vector_db_pipeline\\constants\\__init__.py:]\n",
      "[2024-05-14 14:28:03,677: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:28:03,694: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:28:03,911: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:28:03,911: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 1.000000 seconds:]\n",
      "[2024-05-14 14:28:05,516: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:28:05,527: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:28:05,770: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:28:05,771: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 1.000000 seconds:]\n",
      "[2024-05-14 14:28:07,112: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:28:07,128: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:28:07,144: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:28:07,144: INFO: 1977102676: Initial_file: from pathlib import Path\n",
      "\n",
      "CONFIG_FILE_PATH = Path(\"config/config.yaml\")\n",
      "PARAMS_FILE_PATH = Path(\"params.yaml\")\n",
      "SCHEMA_FILE_PATH = Path(\"schema.yaml\")\n",
      "PROMPT_FILE_PATH = Path(\"prompt_template.yaml\")\n",
      "MODELS_FILE_PATH = Path(\"models.yaml\")\n",
      "IGNORE_FILE_PATH = Path(\"exhalation_ignore.yaml\")\n",
      "\n",
      " \n",
      ":]\n",
      "[2024-05-14 14:28:07,144: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:28:07,144: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:28:07,144: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:28:07,159: INFO: 1977102676: __init__.py summary success:]\n",
      "[2024-05-14 14:28:07,159: INFO: 1977102676: Starting summary of src\\vector_db_pipeline\\constants\\__init___summary.txt:]\n",
      "[2024-05-14 14:28:07,169: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:28:07,194: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:28:07,393: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:28:07,393: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 3.000000 seconds:]\n",
      "[2024-05-14 14:28:12,227: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:28:12,246: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:28:13,493: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:28:13,523: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:28:13,523: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:28:13,539: INFO: 1977102676: Initial_file: **Summary:** This code defines file paths for various configuration files.\n",
      "\n",
      "**Features:**\n",
      "\n",
      "* 5 file paths defined:\n",
      "\t+ CONFIG_FILE_PATH\n",
      "\t+ PARAMS_FILE_PATH\n",
      "\t+ SCHEMA_FILE_PATH\n",
      "\t+ PROMPT_FILE_PATH\n",
      "\t+ MODELS_FILE_PATH \n",
      ":]\n",
      "[2024-05-14 14:28:13,539: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:28:13,539: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:28:13,539: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:28:13,539: INFO: 1977102676: __init___summary.txt summary success:]\n",
      "[2024-05-14 14:28:13,539: INFO: 1977102676: Starting summary of src\\vector_db_pipeline\\entity\\config_entity.py:]\n",
      "[2024-05-14 14:28:13,555: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:28:13,561: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:28:13,811: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:28:13,811: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 2.000000 seconds:]\n",
      "[2024-05-14 14:28:17,113: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:28:17,129: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:28:17,351: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:28:17,353: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 2.000000 seconds:]\n",
      "[2024-05-14 14:28:19,697: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:28:19,713: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:28:19,728: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:28:19,732: INFO: 1977102676: Initial_file: from dataclasses import dataclass\n",
      "from pathlib import Path\n",
      "\n",
      "\n",
      "@dataclass(frozen=True)\n",
      "class DataIngestionConfig:\n",
      "    root_dir: Path\n",
      "    local_data_file: Path\n",
      "    load_dir: Path\n",
      "    text_spliter_config : dict\n",
      "    namespace_idx:str\n",
      "\n",
      "    \n",
      "@dataclass(frozen=True)\n",
      "class DataValidationConfig:\n",
      "    root_dir: Path\n",
      "    read_data_dir: Path\n",
      "    STATUS_FILE: str\n",
      "    SCHEMA: dict\n",
      "\n",
      "@dataclass(frozen=True)\n",
      "class DataUploadConfig:\n",
      "    root_dir: Path\n",
      "    read_data_dir: Path\n",
      "    STATUS_FILE: str\n",
      "    index_info: dict\n",
      "    batch_size: int\n",
      "\n",
      "@dataclass(frozen=True)\n",
      "class CodeStructureConfig:\n",
      "    root_dir: Path\n",
      "    load_struct_dir: Path\n",
      "    load_ignored_dir: Path\n",
      "    gitignore_path: Path\n",
      "    code_dir: Path\n",
      "    sructure_file: Path\n",
      "    models: dict\n",
      "    structure_prompt: str \n",
      ":]\n",
      "[2024-05-14 14:28:19,733: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:28:19,734: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:28:19,736: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:28:19,744: INFO: 1977102676: config_entity.py summary success:]\n",
      "[2024-05-14 14:28:19,745: INFO: 1977102676: Starting summary of src\\vector_db_pipeline\\entity\\config_entity_summary.txt:]\n",
      "[2024-05-14 14:28:19,753: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:28:19,771: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:28:19,988: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:28:19,988: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 3.000000 seconds:]\n",
      "[2024-05-14 14:28:23,918: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:28:23,958: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:28:24,182: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:28:24,182: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 5.000000 seconds:]\n",
      "[2024-05-14 14:28:29,629: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:28:29,651: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:28:29,664: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:28:29,664: INFO: 1977102676: Initial_file: Summary:\n",
      "The code defines four data classes: DataIngestionConfig, DataValidationConfig, DataUploadConfig, and CodeStructureConfig. These classes represent configuration settings for data ingestion, validation, upload, and code structure, respectively.\n",
      "\n",
      "Features:\n",
      "\n",
      "* DataIngestionConfig:\n",
      "\t+ root_dir\n",
      "\t+ local_data_file\n",
      "\t+ load_dir\n",
      "\t+ text_spliter_config\n",
      "\t+ namespace_idx\n",
      "* DataValidationConfig:\n",
      "\t+ root_dir\n",
      "\t+ read_data_dir\n",
      "\t+ STATUS_FILE\n",
      "\t+ SCHEMA\n",
      "* DataUploadConfig:\n",
      "\t+ root_dir\n",
      "\t+ read_data_dir\n",
      "\t+ STATUS_FILE\n",
      "\t+ index_info\n",
      "\t+ batch_size\n",
      "* CodeStructureConfig:\n",
      "\t+ root_dir\n",
      "\t+ load_struct_dir\n",
      "\t+ load_ignored_dir\n",
      "\t+ gitignore_path\n",
      "\t+ code_dir\n",
      "\t+ sructure_file\n",
      "\t+ models\n",
      "\t+ structure_prompt \n",
      ":]\n",
      "[2024-05-14 14:28:29,664: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:28:29,664: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:28:29,664: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:28:29,680: INFO: 1977102676: config_entity_summary.txt summary success:]\n",
      "[2024-05-14 14:28:29,681: INFO: 1977102676: Starting summary of src\\vector_db_pipeline\\entity\\__init__.py:]\n",
      "[2024-05-14 14:28:29,683: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:28:29,685: INFO: 1977102676: config_entity_summary.txt empty.:]\n",
      "[2024-05-14 14:28:29,686: INFO: 1977102676: Starting summary of src\\vector_db_pipeline\\pipeline\\DataIngestion.py:]\n",
      "[2024-05-14 14:28:29,693: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:28:29,717: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:28:29,960: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:28:29,962: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 15.000000 seconds:]\n",
      "[2024-05-14 14:28:46,030: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:28:46,046: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:28:46,266: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:28:46,266: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 3.000000 seconds:]\n",
      "[2024-05-14 14:28:49,647: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:28:49,691: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:28:49,709: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:28:49,712: INFO: 1977102676: Initial_file: from vector_db_pipeline.config.configuration import ConfigurationManager\n",
      "from vector_db_pipeline.components.data_ingestion import TextProcessor\n",
      "from vector_db_pipeline.utils.common import list_files_in_directory, get_json\n",
      "from vector_db_pipeline import logger\n",
      "from pathlib import Path\n",
      "\n",
      "\n",
      "STAGE_NAME = \"Data Ingestion stage\"\n",
      "\n",
      "class DataIngestionPipeline:\n",
      "    def __init__(self):\n",
      "        pass\n",
      "\n",
      "    def main(self):\n",
      "        \"\"\"\n",
      "        Executes the data ingestion pipeline.\n",
      "\n",
      "        Retrieves data ingestion configuration from ConfigurationManager.\n",
      "        Retrieves JSON files from the local data directory specified in the configuration.\n",
      "        Parses JSON files to extract data.\n",
      "        Initializes TextProcessor with data ingestion configuration.\n",
      "        Splits text data into chunks and embeds them.\n",
      "        Saves the processed data into a JSON file.\n",
      "        \"\"\"\n",
      "        # Retrieve data ingestion configuration\n",
      "        config = ConfigurationManager()\n",
      "        data_ingestion_config = config.get_data_ingestion_config()\n",
      "        \n",
      "        # Get JSON files from local data directory\n",
      "        json_files = list_files_in_directory(Path(data_ingestion_config.local_data_file))\n",
      "        \n",
      "        # Parse JSON files to extract data\n",
      "        data = get_json(json_files)\n",
      "        \n",
      "        # Initialize TextProcessor with data ingestion configuration\n",
      "        text_processor = TextProcessor(config=data_ingestion_config)\n",
      "        \n",
      "        # Split text data into chunks and embed them\n",
      "        splited_text_data = text_processor.split_text(data)\n",
      "        \n",
      "        \n",
      "        # Save the processed data into a JSON file\n",
      "        text_processor.load_data_json(splited_text_data)\n",
      "\n",
      "\n",
      "\n",
      "if __name__ =='__main__':\n",
      "    try:\n",
      "        logger.info(f\">>>>>>> stage {STAGE_NAME} started <<<<<<<<<<<<\")\n",
      "        obj = DataIngestionPipeline()\n",
      "        obj.main()\n",
      "        logger.info(f\">>>>>>> stage {STAGE_NAME} completed <<<<<<<<<<<<\\n\\nx===============x\")\n",
      "\n",
      "    except Exception as e:\n",
      "        logger.exception(e)\n",
      "        raise(e) \n",
      ":]\n",
      "[2024-05-14 14:28:49,714: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:28:49,717: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:28:49,720: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:28:49,747: INFO: 1977102676: DataIngestion.py summary success:]\n",
      "[2024-05-14 14:28:49,750: INFO: 1977102676: Starting summary of src\\vector_db_pipeline\\pipeline\\DataIngestion_summary.txt:]\n",
      "[2024-05-14 14:28:49,769: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:28:49,834: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:28:50,060: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:28:50,077: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 7.000000 seconds:]\n",
      "[2024-05-14 14:28:57,960: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:28:57,982: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:28:58,199: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:28:58,202: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 5.000000 seconds:]\n",
      "[2024-05-14 14:29:03,577: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:29:03,599: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:29:03,613: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:29:03,614: INFO: 1977102676: Initial_file: **Summary:**\n",
      "The code defines a data ingestion pipeline that retrieves configuration, reads JSON files, extracts data, processes text, and saves the processed data to a JSON file.\n",
      "\n",
      "**Features:**\n",
      "\n",
      "* Classes:\n",
      "\t+ DataIngestionPipeline\n",
      "\t+ ConfigurationManager\n",
      "\t+ TextProcessor\n",
      "* Methods:\n",
      "\t+ DataIngestionPipeline:\n",
      "\t\t- __init__\n",
      "\t\t- main\n",
      "\t+ ConfigurationManager:\n",
      "\t\t- get_data_ingestion_config\n",
      "\t+ TextProcessor:\n",
      "\t\t- __init__\n",
      "\t\t- split_text\n",
      "\t\t- load_data_json\n",
      "* Functions:\n",
      "\t+ list_files_in_directory\n",
      "\t+ get_json \n",
      ":]\n",
      "[2024-05-14 14:29:03,615: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:29:03,616: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:29:03,617: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:29:03,628: INFO: 1977102676: DataIngestion_summary.txt summary success:]\n",
      "[2024-05-14 14:29:03,631: INFO: 1977102676: Starting summary of src\\vector_db_pipeline\\pipeline\\DataUpload.py:]\n",
      "[2024-05-14 14:29:03,632: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:29:03,650: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:29:03,884: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:29:03,884: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 15.000000 seconds:]\n",
      "[2024-05-14 14:29:20,028: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:29:20,046: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:29:20,277: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:29:20,278: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 5.000000 seconds:]\n",
      "[2024-05-14 14:29:25,681: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:29:25,702: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:29:25,715: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:29:25,715: INFO: 1977102676: Initial_file: from vector_db_pipeline.config.configuration import ConfigurationManager\n",
      "from vector_db_pipeline.components.data_load import DataUpload\n",
      "from vector_db_pipeline import logger\n",
      "\n",
      "STAGE_NAME = \"Data Upload stage\"\n",
      "\n",
      "class DataUploadPipeline:\n",
      "    def __init__(self, should_restart_database=False):\n",
      "        \"\"\"\n",
      "        Initializes the DataUploadPipeline.\n",
      "\n",
      "        Retrieves data upload configuration using ConfigurationManager.\n",
      "        \"\"\"\n",
      "        self.config_manager = ConfigurationManager()\n",
      "        self.data_upload_config = self.config_manager.get_data_upload_config()\n",
      "        self.should_restart_database = should_restart_database\n",
      "\n",
      "    def restart_database(self):\n",
      "        \"\"\"\n",
      "        Restarts the database by deleting and recreating the index.\n",
      "\n",
      "        Deletes the existing index and recreates it to start fresh.\n",
      "        \"\"\"\n",
      "        data_upload = DataUpload(config=self.data_upload_config)\n",
      "        data_upload.del_index()\n",
      "        data_upload.recreate_index()\n",
      "\n",
      "    def main(self):\n",
      "        \"\"\"\n",
      "        Executes the data upload pipeline.\n",
      "\n",
      "        Initializes DataUpload with data upload configuration.\n",
      "        Generates Pinecone vectors from the input data.\n",
      "        Uploads vectors to the Pinecone index in batches.\n",
      "        \"\"\"\n",
      "        # Initialize DataUpload with data upload configuration\n",
      "        data_upload = DataUpload(config=self.data_upload_config)\n",
      "        \n",
      "        # Restart the database if needed\n",
      "        if self.should_restart_database:\n",
      "            logger.info(f\"Restarting database\")\n",
      "            self.restart_database()\n",
      "\n",
      "        # Generate Pinecone vectors from the input data\n",
      "        pinecone_vector = data_upload.pinecon_vector()\n",
      "        \n",
      "        # Upload vectors to the Pinecone index in batches\n",
      "        data_upload.batch_upload(pinecone_vector)\n",
      "\n",
      "\n",
      "if __name__ =='__main__':\n",
      "    try:\n",
      "        logger.info(f\">>>>>>> stage {STAGE_NAME} started <<<<<<<<<<<<\")\n",
      "        obj = DataUploadPipeline(should_restart_database=True)\n",
      "        obj.restart_database()\n",
      "        obj.main()\n",
      "        logger.info(f\">>>>>>> stage {STAGE_NAME} completed <<<<<<<<<<<<\\n\\nx===============x\")\n",
      "\n",
      "    except Exception as e:\n",
      "        logger.exception(e)\n",
      "        raise(e) \n",
      ":]\n",
      "[2024-05-14 14:29:25,718: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:29:25,719: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:29:25,720: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:29:25,730: INFO: 1977102676: DataUpload.py summary success:]\n",
      "[2024-05-14 14:29:25,731: INFO: 1977102676: Starting summary of src\\vector_db_pipeline\\pipeline\\DataUpload_summary.txt:]\n",
      "[2024-05-14 14:29:25,744: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:29:25,766: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:29:25,996: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:29:25,996: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 9.000000 seconds:]\n",
      "[2024-05-14 14:29:35,746: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:29:35,770: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:29:35,995: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:29:35,995: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 3.000000 seconds:]\n",
      "[2024-05-14 14:29:39,350: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:29:39,369: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:29:39,383: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:29:39,384: INFO: 1977102676: Initial_file: **Summary:**\n",
      "The code defines a `DataUploadPipeline` class that handles data upload to a Pinecone index. It initializes with a configuration, restarts the database if needed, generates Pinecone vectors from input data, and uploads the vectors to the index in batches.\n",
      "\n",
      "**Functions:**\n",
      "\n",
      "* `__init__`: Initializes the `DataUploadPipeline` instance with a configuration and an optional flag to restart the database.\n",
      "* `restart_database`: Restarts the database by deleting and recreating the index.\n",
      "* `main`: Executes the data upload pipeline, including generating Pinecone vectors and uploading them to the index.\n",
      "\n",
      "**Classes:**\n",
      "\n",
      "* `DataUploadPipeline`: The main class that handles the data upload pipeline.\n",
      "* `DataUpload`: A component used for data upload, instantiated with a configuration.\n",
      "\n",
      "**Methods:**\n",
      "\n",
      "* `get_data_upload_config`: Retrieves data upload configuration using `ConfigurationManager`.\n",
      "* `del_index`: Deletes the existing index.\n",
      "* `recreate_index`: Recreates the index.\n",
      "* `pinecon_vector`: Generates Pinecone vectors from input data.\n",
      "* `batch_upload`: Uploads vectors to the Pinecone index in batches. \n",
      ":]\n",
      "[2024-05-14 14:29:39,386: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:29:39,387: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:29:39,387: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:29:39,397: INFO: 1977102676: DataUpload_summary.txt summary success:]\n",
      "[2024-05-14 14:29:39,398: INFO: 1977102676: Starting summary of src\\vector_db_pipeline\\pipeline\\DataValidation.py:]\n",
      "[2024-05-14 14:29:39,403: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:29:39,428: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:29:39,662: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:29:39,665: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 11.000000 seconds:]\n",
      "[2024-05-14 14:29:51,529: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:29:51,550: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:29:51,772: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:29:51,772: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 1.000000 seconds:]\n",
      "[2024-05-14 14:29:53,101: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:29:53,113: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:29:53,129: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:29:53,129: INFO: 1977102676: Initial_file: from vector_db_pipeline.config.configuration import ConfigurationManager\n",
      "from vector_db_pipeline.components.data_validation import DataValidation\n",
      "from vector_db_pipeline import logger\n",
      "\n",
      "\n",
      "STAGE_NAME = \"Data Validation stage\"\n",
      "\n",
      "\n",
      "class DataValidationPipeline:\n",
      "    def __init__(self):\n",
      "        pass\n",
      "\n",
      "    def main(self):\n",
      "        \"\"\"\n",
      "        Executes the data validation pipeline.\n",
      "\n",
      "        Instantiates the ConfigurationManager to retrieve data validation configuration.\n",
      "        Initializes DataValidation with the retrieved configuration.\n",
      "        Executes data validation checks.\n",
      "        \"\"\"\n",
      "        config = ConfigurationManager()\n",
      "        data_validation_config = config.get_data_validation_config()\n",
      "        data_validation = DataValidation(config=data_validation_config)\n",
      "        data_validation.validate_all_columns()\n",
      "        data_validation.validate_unique_index()\n",
      "        data_validation.validate_column_types()\n",
      "\n",
      "\n",
      "if __name__ =='__main__':\n",
      "    try:\n",
      "        logger.info(f\">>>>>>> stage {STAGE_NAME} started <<<<<<<<<<<<\")\n",
      "        obj = DataValidationPipeline()\n",
      "        obj.main()\n",
      "        logger.info(f\">>>>>>> stage {STAGE_NAME} completed <<<<<<<<<<<<\\n\\nx===============x\")\n",
      "\n",
      "    except Exception as e:\n",
      "        logger.exception(e)\n",
      "        raise(e) \n",
      ":]\n",
      "[2024-05-14 14:29:53,129: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:29:53,129: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:29:53,129: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:29:53,129: INFO: 1977102676: DataValidation.py summary success:]\n",
      "[2024-05-14 14:29:53,129: INFO: 1977102676: Starting summary of src\\vector_db_pipeline\\pipeline\\DataValidation_summary.txt:]\n",
      "[2024-05-14 14:29:53,149: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:29:53,163: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:29:53,429: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:29:53,430: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 8.000000 seconds:]\n",
      "[2024-05-14 14:30:02,184: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:30:02,203: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:30:02,425: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:30:02,430: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 4.000000 seconds:]\n",
      "[2024-05-14 14:30:06,783: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:30:06,801: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:30:06,816: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:30:06,816: INFO: 1977102676: Initial_file: **Summary:**\n",
      "The code defines a data validation pipeline that retrieves configuration, initializes a data validation component, and executes validation checks on data columns and types.\n",
      "\n",
      "**Functions:**\n",
      "\n",
      "* `main` (method of `DataValidationPipeline` class)\n",
      "\n",
      "**Classes:**\n",
      "\n",
      "* `DataValidationPipeline`\n",
      "\n",
      "**Methods:**\n",
      "\n",
      "* `__init__` (initializer of `DataValidationPipeline` class)\n",
      "* `main` (method of `DataValidationPipeline` class)\n",
      "* `get_data_validation_config` (method of `ConfigurationManager` class)\n",
      "* `validate_all_columns` (method of `DataValidation` class)\n",
      "* `validate_unique_index` (method of `DataValidation` class)\n",
      "* `validate_column_types` (method of `DataValidation` class) \n",
      ":]\n",
      "[2024-05-14 14:30:06,816: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:30:06,816: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:30:06,816: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:30:06,830: INFO: 1977102676: DataValidation_summary.txt summary success:]\n",
      "[2024-05-14 14:30:06,831: INFO: 1977102676: Starting summary of src\\vector_db_pipeline\\pipeline\\GenerateAppStructure.py:]\n",
      "[2024-05-14 14:30:06,833: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:30:06,853: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:30:07,084: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:30:07,084: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 13.000000 seconds:]\n",
      "[2024-05-14 14:30:21,105: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:30:21,130: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:30:21,364: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:30:21,364: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 2.000000 seconds:]\n",
      "[2024-05-14 14:30:23,753: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:30:23,780: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:30:23,791: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:30:23,791: INFO: 1977102676: Initial_file: from vector_db_pipeline.config.configuration import ConfigurationManager\n",
      "from vector_db_pipeline.components.generate_app_structure import CodeStructure\n",
      "from vector_db_pipeline import logger\n",
      "from pathlib import Path\n",
      "\n",
      "STAGE_NAME = \"Getting App File Structure stage\"\n",
      "\n",
      "\"\"\"\n",
      "Generates the application structure pipeline.\n",
      "\n",
      "Methods:\n",
      "    main(): Main method to execute the pipeline.\n",
      "\"\"\"\n",
      "class GenerateAppStructurePipeline:\n",
      "    def __init__(self):\n",
      "        \"\"\"\n",
      "        Initializes the GenerateAppStructurePipeline.\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "    def main(self):\n",
      "        \"\"\"\n",
      "        Executes the main pipeline to generate the application structure.\n",
      "\n",
      "        Returns:\n",
      "            None\n",
      "        \"\"\"\n",
      "        # Initialize ConfigurationManager\n",
      "        config = ConfigurationManager()\n",
      "        \n",
      "        # Get code structure configuration\n",
      "        code_structure_config = config.get_code_structure_config()\n",
      "        \n",
      "        # Initialize CodeStructure with code structure configuration\n",
      "        get_code_structure = CodeStructure(config=code_structure_config)\n",
      "        \n",
      "        # Retrieve ignored subdirectories from .gitignore\n",
      "        get_code_structure.get_ignored_subdirs_from_gitignore()\n",
      "        \n",
      "        # Build directory structure\n",
      "        directory_structure = get_code_structure.build_directory_structure()\n",
      "        \n",
      "        # Format the directory structure\n",
      "        get_code_structure.get_formated_strcuture(directory_structure)\n",
      "\n",
      "\n",
      "if __name__ =='__main__':\n",
      "    try:\n",
      "        logger.info(f\">>>>>>> stage {STAGE_NAME} started <<<<<<<<<<<<\")\n",
      "        obj = GenerateAppStructurePipeline()\n",
      "        obj.main()\n",
      "        logger.info(f\">>>>>>> stage {STAGE_NAME} completed <<<<<<<<<<<<\\n\\nx===============x\")\n",
      "\n",
      "    except Exception as e:\n",
      "        logger.exception(e)\n",
      "        raise(e) \n",
      ":]\n",
      "[2024-05-14 14:30:23,791: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:30:23,796: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:30:23,797: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:30:23,806: INFO: 1977102676: GenerateAppStructure.py summary success:]\n",
      "[2024-05-14 14:30:23,808: INFO: 1977102676: Starting summary of src\\vector_db_pipeline\\pipeline\\GenerateAppStructure_summary.txt:]\n",
      "[2024-05-14 14:30:23,820: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:30:23,834: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:30:24,069: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:30:24,069: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 8.000000 seconds:]\n",
      "[2024-05-14 14:30:32,971: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:30:32,981: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:30:33,229: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:30:33,231: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 5.000000 seconds:]\n",
      "[2024-05-14 14:30:38,623: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:30:38,644: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:30:38,649: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:30:38,649: INFO: 1977102676: Initial_file: **Summary:**\n",
      "The code defines a pipeline to generate an application structure. It initializes a configuration manager, retrieves code structure configuration, builds a directory structure, and formats it.\n",
      "\n",
      "**Functions:**\n",
      "\n",
      "* `main()`: Executes the pipeline to generate the application structure.\n",
      "\n",
      "**Classes:**\n",
      "\n",
      "* `GenerateAppStructurePipeline`: Initializes the pipeline and executes the main method.\n",
      "\n",
      "**Methods:**\n",
      "\n",
      "* `__init__()`: Initializes the `GenerateAppStructurePipeline` class.\n",
      "* `main()`: Executes the pipeline to generate the application structure.\n",
      "\n",
      "**Dependencies:**\n",
      "\n",
      "* `vector_db_pipeline.config.configuration`: Imported `ConfigurationManager` class.\n",
      "* `vector_db_pipeline.components.generate_app_structure`: Imported `CodeStructure` class.\n",
      "* `vector_db_pipeline`: Imported `logger` module.\n",
      "* `pathlib`: Imported `Path` class. \n",
      ":]\n",
      "[2024-05-14 14:30:38,649: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:30:38,649: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:30:38,665: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:30:38,674: INFO: 1977102676: GenerateAppStructure_summary.txt summary success:]\n",
      "[2024-05-14 14:30:38,675: INFO: 1977102676: Starting summary of src\\vector_db_pipeline\\pipeline\\__init__.py:]\n",
      "[2024-05-14 14:30:38,678: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:30:38,680: INFO: 1977102676: GenerateAppStructure_summary.txt empty.:]\n",
      "[2024-05-14 14:30:38,681: INFO: 1977102676: Starting summary of src\\vector_db_pipeline\\utils\\common.py:]\n",
      "[2024-05-14 14:30:38,682: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:30:38,705: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:30:38,947: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:30:38,948: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 33.000000 seconds:]\n",
      "[2024-05-14 14:31:14,104: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:31:14,124: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:31:14,338: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:31:14,338: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 11.000000 seconds:]\n",
      "[2024-05-14 14:31:25,846: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:31:25,866: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:31:25,879: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:31:25,879: INFO: 1977102676: Initial_file: import os\n",
      "from box.exceptions import BoxValueError\n",
      "import yaml\n",
      "from vector_db_pipeline import logger\n",
      "import json\n",
      "import joblib\n",
      "from ensure import ensure_annotations\n",
      "from box import ConfigBox\n",
      "from pathlib import Path\n",
      "from typing import Any, List\n",
      "\n",
      "\n",
      "\n",
      "@ensure_annotations\n",
      "def read_yaml(path_to_yaml: Path) -> ConfigBox:\n",
      "    \"\"\"reads yaml file and returns\n",
      "\n",
      "    Args:\n",
      "        path_to_yaml (str): path like input\n",
      "\n",
      "    Raises:\n",
      "        ValueError: if yaml file is empty\n",
      "        e: empty file\n",
      "\n",
      "    Returns:\n",
      "        ConfigBox: ConfigBox type\n",
      "    \"\"\"\n",
      "    try:\n",
      "        with open(path_to_yaml) as yaml_file:\n",
      "            content = yaml.safe_load(yaml_file)\n",
      "            logger.info(f\"yaml file: {path_to_yaml} loaded successfully\")\n",
      "            return ConfigBox(content)\n",
      "    except BoxValueError:\n",
      "        raise ValueError(\"yaml file is empty\")\n",
      "    except Exception as e:\n",
      "        raise e\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "@ensure_annotations\n",
      "def create_directories(path_to_directories: list, verbose=True):\n",
      "    \"\"\"Create a list of directories if they do not exist.\n",
      "\n",
      "    Args:\n",
      "        path_to_directories (list): List of paths of directories.\n",
      "        verbose (bool, optional): Whether to log messages. Defaults to True.\n",
      "    \"\"\"\n",
      "    for path in path_to_directories:\n",
      "        if not os.path.exists(path):  # Check if directory already exists\n",
      "            os.makedirs(path, exist_ok=True)\n",
      "            if verbose:\n",
      "                logger.info(f\"Created directory at: {path}\")\n",
      "        elif verbose:\n",
      "            logger.info(f\"Directory already exists: {path}\")\n",
      "\n",
      "\n",
      "\n",
      "@ensure_annotations\n",
      "def save_json(path: Path, data: dict):\n",
      "    \"\"\"save json data\n",
      "\n",
      "    Args:\n",
      "        path (Path): path to json file\n",
      "        data (dict): data to be saved in json file\n",
      "    \"\"\"\n",
      "    with open(path, \"w\") as f:\n",
      "        json.dump(data, f, indent=4)\n",
      "\n",
      "    logger.info(f\"json file saved at: {path}\")\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "@ensure_annotations\n",
      "def load_json(path: Path) -> ConfigBox:\n",
      "    \"\"\"load json files data\n",
      "\n",
      "    Args:\n",
      "        path (Path): path to json file\n",
      "\n",
      "    Returns:\n",
      "        ConfigBox: data as class attributes instead of dict\n",
      "    \"\"\"\n",
      "    with open(path) as f:\n",
      "        content = json.load(f)\n",
      "\n",
      "    logger.info(f\"json file loaded succesfully from: {path}\")\n",
      "    return ConfigBox(content)\n",
      "\n",
      "\n",
      "@ensure_annotations\n",
      "def save_bin(data: Any, path: Path):\n",
      "    \"\"\"save binary file\n",
      "\n",
      "    Args:\n",
      "        data (Any): data to be saved as binary\n",
      "        path (Path): path to binary file\n",
      "    \"\"\"\n",
      "    joblib.dump(value=data, filename=path)\n",
      "    logger.info(f\"binary file saved at: {path}\")\n",
      "\n",
      "\n",
      "@ensure_annotations\n",
      "def load_bin(path: Path) -> Any:\n",
      "    \"\"\"load binary data\n",
      "\n",
      "    Args:\n",
      "        path (Path): path to binary file\n",
      "\n",
      "    Returns:\n",
      "        Any: object stored in the file\n",
      "    \"\"\"\n",
      "    data = joblib.load(path)\n",
      "    logger.info(f\"binary file loaded from: {path}\")\n",
      "    return data\n",
      "\n",
      "\n",
      "\n",
      "@ensure_annotations\n",
      "def get_size(path: Path) -> str:\n",
      "    \"\"\"get size in KB\n",
      "\n",
      "    Args:\n",
      "        path (Path): path of the file\n",
      "\n",
      "    Returns:\n",
      "        str: size in KB\n",
      "    \"\"\"\n",
      "    size_in_kb = round(os.path.getsize(path)/1024)\n",
      "    return f\"~ {size_in_kb} KB\"\n",
      "\n",
      "\n",
      "@ensure_annotations\n",
      "def get_json(json_files: List) -> List:\n",
      "    \"\"\"\n",
      "    Load data from JSON files and return as a ConfigBox object.\n",
      "\n",
      "    Args:\n",
      "        json_files (List[str]): List of paths to JSON files.\n",
      "\n",
      "    Returns:\n",
      "        ConfigBox: Data as class attributes instead of dict.\n",
      "    \"\"\"\n",
      "    # Initialize an empty list to store data from JSON files\n",
      "    all_data = []\n",
      "\n",
      "    # Iterate through each JSON file\n",
      "    for json_file in json_files:\n",
      "        # Open the JSON file\n",
      "        with open(json_file, 'r', encoding='utf-8') as file:\n",
      "            # Load the JSON data\n",
      "            data = json.load(file)\n",
      "        # Append the loaded data to the list\n",
      "        all_data.append(data)\n",
      "    \n",
      "    # Flatten the list of lists into a single list\n",
      "    flattened_list = [item for sublist in all_data for item in sublist]\n",
      "    \n",
      "    # Log a message indicating successful loading of JSON files\n",
      "    logger.info(f\"JSON files loaded successfully from: {json_files}\")\n",
      "    \n",
      "    # Return the data as a ConfigBox object\n",
      "    return flattened_list\n",
      "\n",
      "@ensure_annotations\n",
      "def list_files_in_directory(path: Path) -> List:\n",
      "    \"\"\"\n",
      "    List files in a directory and its subdirectories.\n",
      "\n",
      "    Args:\n",
      "        path (Path): Path to the directory.\n",
      "\n",
      "    Returns:\n",
      "        List[str]: List of file paths.\n",
      "    \"\"\"\n",
      "    scraped_files = []\n",
      "    for root, dirs, files in os.walk(path):\n",
      "        for file_name in files:\n",
      "            file_path = os.path.join(root, file_name)\n",
      "            scraped_files.append(file_path)\n",
      "    logger.info(f\"Files list successfully loaded from: {path}\")\n",
      "    return scraped_files\n",
      "\n",
      "\n",
      "def get_column_types(df) -> dict:\n",
      "    \"\"\"\n",
      "    Get the types of values in each column of the DataFrame.\n",
      "\n",
      "    Args:\n",
      "        df (DataFrame): The input DataFrame.\n",
      "\n",
      "    Returns:\n",
      "        dict: A dictionary where keys are column names and values are lists containing the types of values present in each column.\n",
      "    \"\"\"\n",
      "\n",
      "    column_types = {}\n",
      "    for column in df.columns:\n",
      "        # Create a set of unique types for values in the column\n",
      "        types = set(type(value) for value in df[column])\n",
      "        # Convert the set to a list for easier handling\n",
      "        types_list = list(types)\n",
      "        \n",
      "        column_types[column] = types_list\n",
      "\n",
      "    return column_types\n",
      "@ensure_annotations\n",
      "def set_to_txt(file_path: Path,set_obj:set):\n",
      "    with open(file_path, \"w\") as file:\n",
      "        # Convert the set elements to strings and write them to the file\n",
      "        for element in set_obj:\n",
      "            file.write(str(element) + \"\\n\")\n",
      "\n",
      "\n",
      "@ensure_annotations\n",
      "def read_txt(file_path: Path):\n",
      "    with open(file_path, 'r') as file:\n",
      "              content = file.read()\n",
      "    return content\n",
      "@ensure_annotations\n",
      "def remove_extension(file_name: Path):\n",
      "    base_name, extension = os.path.splitext(file_name)\n",
      "    return base_name\n",
      "\n",
      "@ensure_annotations\n",
      "def write_to_txt(file_path: Path, content):\n",
      "    with open(file_path, 'w') as f:\n",
      "        f.write(content) \n",
      ":]\n",
      "[2024-05-14 14:31:25,882: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:31:25,883: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:31:25,884: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:31:25,893: INFO: 1977102676: common.py summary success:]\n",
      "[2024-05-14 14:31:25,894: INFO: 1977102676: Starting summary of src\\vector_db_pipeline\\utils\\common_summary.txt:]\n",
      "[2024-05-14 14:31:25,915: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:31:25,936: INFO: 1977102676: Creating JSON summary draft:]\n",
      "[2024-05-14 14:31:26,171: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:31:26,171: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 13.000000 seconds:]\n",
      "[2024-05-14 14:31:40,868: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:31:40,894: INFO: 1977102676: Checking JSON draft data type:]\n",
      "[2024-05-14 14:31:41,115: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\":]\n",
      "[2024-05-14 14:31:41,116: INFO: _base_client: Retrying request to /openai/v1/chat/completions in 11.000000 seconds:]\n",
      "[2024-05-14 14:31:52,622: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-05-14 14:31:52,646: INFO: 1977102676: JSON summary draft correct and assigned to final_json_summary:]\n",
      "[2024-05-14 14:31:52,665: INFO: 1977102676: ---STATE PRINTER---:]\n",
      "[2024-05-14 14:31:52,665: INFO: 1977102676: Initial_file: Summary:\n",
      "This is a Python script that provides various utility functions for file operations, data loading, and data manipulation.\n",
      "\n",
      "Functions:\n",
      "\n",
      "1. `read_yaml`: Reads a YAML file and returns a `ConfigBox` object.\n",
      "2. `create_directories`: Creates a list of directories if they do not exist.\n",
      "3. `save_json`: Saves data to a JSON file.\n",
      "4. `load_json`: Loads data from a JSON file and returns a `ConfigBox` object.\n",
      "5. `save_bin`: Saves binary data to a file.\n",
      "6. `load_bin`: Loads binary data from a file.\n",
      "7. `get_size`: Returns the size of a file in KB.\n",
      "8. `get_json`: Loads data from multiple JSON files and returns a list of `ConfigBox` objects.\n",
      "9. `list_files_in_directory`: Lists files in a directory and its subdirectories.\n",
      "10. `get_column_types`: Returns a dictionary with column names as keys and lists of value types as values for a given DataFrame.\n",
      "11. `set_to_txt`: Writes a set of elements to a text file.\n",
      "12. `read_txt`: Reads the content of a text file.\n",
      "13. `remove_extension`: Removes the extension from a file name.\n",
      "14. `write_to_txt`: Writes content to a text file.\n",
      "\n",
      "Classes:\n",
      "\n",
      "* `ConfigBox`: A class used to store data as class attributes instead of a dictionary.\n",
      "\n",
      "Methods:\n",
      "\n",
      "* `yaml.safe_load()`: Loads YAML data from a file.\n",
      "* `json.dump()`: Dumps data to a JSON file.\n",
      "* `json.load()`: Loads data from a JSON file.\n",
      "* `joblib.dump()`: Dumps binary data to a file.\n",
      "* `joblib.load()`: Loads binary data from a file.\n",
      "* `os.path.exists()`: Checks if a directory exists.\n",
      "* `os.makedirs()`: Creates a directory if it does not exist.\n",
      "* `os.path.getsize()`: Returns the size of a file in bytes.\n",
      "* `os.path.splitext()`: Splits a file name into a base name and extension. \n",
      ":]\n",
      "[2024-05-14 14:31:52,667: INFO: 1977102676: Draft Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:31:52,669: INFO: 1977102676: Final Json Summary: <class 'dict'> \n",
      ":]\n",
      "[2024-05-14 14:31:52,669: INFO: 1977102676: Num Steps: 2 \n",
      ":]\n",
      "[2024-05-14 14:31:52,686: INFO: 1977102676: common_summary.txt summary success:]\n",
      "[2024-05-14 14:31:52,688: INFO: 1977102676: Starting summary of src\\vector_db_pipeline\\utils\\__init__.py:]\n",
      "[2024-05-14 14:31:52,691: INFO: 1977102676: File content successfully read using encoding: utf-8:]\n",
      "[2024-05-14 14:31:52,692: INFO: 1977102676: common_summary.txt empty.:]\n",
      "[2024-05-14 14:31:52,697: INFO: common: json file saved at: artifacts\\json_summary\\json_summary.json:]\n",
      "[2024-05-14 14:31:52,699: INFO: 1977102676: JSON summaries loaded to artifacts/json_summary/json_summary.json:]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "json_summary_config = ConfigurationManager()\n",
    "config_json = json_summary_config.get_json_summary_config()\n",
    "generate_json_summary = JsonSummary(config_json)\n",
    "files_in_app = generate_json_summary.files_in_app()\n",
    "generate_json_summary.configure_model_system()\n",
    "\n",
    "app = generate_json_summary.create_graph_agents(StateGraph(GraphState))\n",
    "\n",
    "generate_json_summary.run_graph_agents(app,files_in_app)\n",
    "logger.info(f\"Plot comparision latency: {(time.time() - start):.4f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
