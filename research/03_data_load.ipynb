{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Maza\\\\Desktop\\\\vector_db_pipeline'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from vector_db_pipeline.utils.common import read_yaml, create_directories\n",
    "from vector_db_pipeline.constants import *\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataUploadConfig:\n",
    "    root_dir: Path\n",
    "    read_data_dir: Path\n",
    "    STATUS_FILE: str\n",
    "    index_info: dict\n",
    "    batch_size: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "        \n",
    "\n",
    "    \n",
    "    def get_data_upload_config(self) -> DataUploadConfig:\n",
    "        config = self.config.data_load\n",
    "        index_info = self.params.INDEX_INFO\n",
    "        batch_size = self.params.BATCH_SIZE\n",
    "    \n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_upload_config = DataUploadConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            read_data_dir=config.read_data_dir,\n",
    "            STATUS_FILE=config.STATUS_FILE,\n",
    "            index_info=index_info,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        return data_upload_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from pinecone import Pinecone, PodSpec\n",
    "import math\n",
    "from dotenv import load_dotenv\n",
    "from vector_db_pipeline.utils.common import read_yaml, create_directories\n",
    "from vector_db_pipeline.constants import *\n",
    "from vector_db_pipeline import logger\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Handles data upload to Pinecone indexes.\n",
    "\n",
    "Attributes:\n",
    "    config (DataUploadConfig): Configuration object containing settings for data upload.\n",
    "    params (dict): Dictionary containing parameters required for data upload.\n",
    "\n",
    "Methods:\n",
    "    del_index(): Deletes the specified index if it exists.\n",
    "    recreate_index(): Recreates the index with specified dimensions, metric, and environment.\n",
    "    pinecon_vector(): Converts data from CSV to a list of JSON objects.\n",
    "    batch_upload(pinecone_vector): Uploads vectors to a Pinecone index in batches.\n",
    "\"\"\"\n",
    "class DataUpload:\n",
    "    def __init__(self, config: DataUploadConfig):\n",
    "        \"\"\"\n",
    "        Initializes DataUpload class with provided configuration and parameters.\n",
    "\n",
    "        Args:\n",
    "            config (DataUploadConfig): Configuration object containing settings for data upload.\n",
    "            params_filepath (str): Filepath to parameters file. Defaults to PARAMS_FILE_PATH.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        #initialize db\n",
    "        load_dotenv()\n",
    "        pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "        self.pc = Pinecone(api_key=pinecone_api_key)\n",
    "        self.index_info = self.config.index_info\n",
    "        self.index_name = self.index_info.INDEX_NAME\n",
    "        \n",
    "        \n",
    "     \n",
    "    def del_index(self):\n",
    "        \"\"\"\n",
    "        Deletes the specified index if it exists.\n",
    "        \"\"\"\n",
    "        if self.index_name in [index_info[\"name\"] for index_info in self.pc.list_indexes()]:\n",
    "            self.pc.delete_index(self.index_name)\n",
    "            logger.info(f\"Index '{self.index_name}' deleted \")\n",
    "\n",
    "    def recreate_index(self):\n",
    "        \"\"\"\n",
    "        Recreates the index with specified dimensions, metric, and environment.\n",
    "        \"\"\"\n",
    "\n",
    "        dim = self.index_info.DIMENSIONS\n",
    "        met = self.index_info.METRIC\n",
    "        env = self.index_info.ENVIROMENT\n",
    "        existing_indexes = [index_info[\"name\"] for index_info in self.pc.list_indexes()]\n",
    "        \n",
    "        # Check if index already exists\n",
    "        if self.index_name not in existing_indexes:\n",
    "            # Create index if it doesn't exist\n",
    "            self.pc.create_index(\n",
    "                name=self.index_name,\n",
    "                dimension=dim,\n",
    "                metric=met,\n",
    "                spec=PodSpec(\n",
    "                    environment=env\n",
    "                )\n",
    "            )\n",
    "            # Wait for index to be initialized\n",
    "            while not self.pc.describe_index(self.index_name).status['ready']:\n",
    "                time.sleep(1)\n",
    "        index = self.pc.Index(self.index_name)\n",
    "        logger.info(\"Index created\")\n",
    "        logger.info(index.describe_index_stats())\n",
    "\n",
    "    def pinecon_vector(self): \n",
    "        \"\"\"\n",
    "        Converts data from CSV to a list of JSON objects.\n",
    "\n",
    "        Returns:\n",
    "            pinecone_vect (list): List of JSON objects representing each row of the dataframe.\n",
    "        \"\"\"\n",
    "        data_read_path = self.config.read_data_dir\n",
    "        df = pd.read_csv(data_read_path)\n",
    "        pinecone_vect = []\n",
    "        \n",
    "        for i, row in df.iterrows():\n",
    "            id = row['id']\n",
    "            values = row['values'][1:-1].split(',')\n",
    "            vector_floats = [float(element) for element in values]  #FIX THIS ##########################################################################################################\n",
    "            text = row['text']\n",
    "            host = row['host']\n",
    "            page_title = row['page_title']\n",
    "            url = row['url']\n",
    "            # Create a dictionary for the metadata containing 'text', 'host', 'page_title', and 'url'\n",
    "            metadata = {'text': text, 'host': host, 'page_title': page_title, 'url': url}\n",
    "            # Create a dictionary for the JSON object containing 'id', 'values', and 'metadata'\n",
    "            emb_vect = {'id': id, 'values': vector_floats, 'metadata': metadata}\n",
    "            \n",
    "            pinecone_vect.append(emb_vect)\n",
    "        logger.info(f\"Data ready for upload\")\n",
    "        with open(self.config.STATUS_FILE, 'a') as f:\n",
    "            f.write(f\"Data size: {len(pinecone_vect)}\\n\")\n",
    "        # Return the list of JSON objects\n",
    "        return pinecone_vect\n",
    "\n",
    "    def batch_upload(self, pinecone_vector):\n",
    "        \"\"\"\n",
    "        Uploads vectors to a Pinecone index in batches.\n",
    "\n",
    "        Args:\n",
    "            pinecone_vector (list): List of JSON objects representing vectors to be uploaded.\n",
    "        \"\"\"\n",
    "        # Determine the batch size and total number of data points\n",
    "        batch_size = self.config.batch_size.BATCH_SIZE \n",
    "        \n",
    "        index = self.pc.Index(self.index_name)\n",
    "        data_size = len(pinecone_vector)\n",
    "        \n",
    "        \n",
    "        # Calculate the number of batches required\n",
    "        batch_num = math.ceil(data_size / batch_size)\n",
    "        logger.info(f\"Uploading: {data_size} vectors, in {batch_num} batches\")\n",
    "        \n",
    "        # Iterate over each batch\n",
    "        for i in range(batch_num):\n",
    " \n",
    "            try:\n",
    "                # Calculate the start and end indices for the current batch\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = min((i + 1) * batch_size, len(pinecone_vector))\n",
    "            \n",
    "                batch_vectors = pinecone_vector[start_idx:end_idx]\n",
    "                \n",
    "                # Upload the vectors to the Pinecone index\n",
    "                index.upsert(vectors=batch_vectors)\n",
    "                logger.info(f\"Batch {i+1} uploaded\")\n",
    "            except Exception as e:\n",
    "                logger.info(f\"Error encountered: {e}\")\n",
    "        \n",
    "        time.sleep(30)\n",
    "        logger.info(index.describe_index_stats())\n",
    "        with open(self.config.STATUS_FILE, 'a') as f:\n",
    "            f.write(f\"Data upload completed\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-09 20:41:42,467: INFO: common: yaml file: config\\config.yaml loaded successfully:]\n",
      "[2024-04-09 20:41:42,469: INFO: common: yaml file: schema.yaml loaded successfully:]\n",
      "[2024-04-09 20:41:42,471: INFO: common: Directory already exists: artifacts/data_upload:]\n",
      "[2024-04-09 20:41:42,473: INFO: common: yaml file: params.yaml loaded successfully:]\n",
      "[2024-04-09 20:41:50,829: INFO: 2024372759: Index 'meshlennysnews' deleted :]\n",
      "[2024-04-09 20:41:58,719: INFO: 2024372759: Index created:]\n",
      "[2024-04-09 20:41:59,292: INFO: 2024372759: {'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {},\n",
      " 'total_vector_count': 0}:]\n",
      "[2024-04-09 20:42:07,219: INFO: 2024372759: Data ready for upload:]\n",
      "[2024-04-09 20:42:07,364: INFO: 2024372759: Uploading: 6124 vectors, in 52 batches:]\n",
      "0\n",
      "120\n",
      "[2024-04-09 20:42:09,095: INFO: 2024372759: Batch 0 uploaded:]\n",
      "120\n",
      "240\n",
      "[2024-04-09 20:42:10,438: INFO: 2024372759: Batch 1 uploaded:]\n",
      "240\n",
      "360\n",
      "[2024-04-09 20:42:11,708: INFO: 2024372759: Batch 2 uploaded:]\n",
      "360\n",
      "480\n",
      "[2024-04-09 20:42:12,986: INFO: 2024372759: Batch 3 uploaded:]\n",
      "480\n",
      "600\n",
      "[2024-04-09 20:42:14,282: INFO: 2024372759: Batch 4 uploaded:]\n",
      "600\n",
      "720\n",
      "[2024-04-09 20:42:15,572: INFO: 2024372759: Batch 5 uploaded:]\n",
      "720\n",
      "840\n",
      "[2024-04-09 20:42:16,882: INFO: 2024372759: Batch 6 uploaded:]\n",
      "840\n",
      "960\n",
      "[2024-04-09 20:42:18,149: INFO: 2024372759: Batch 7 uploaded:]\n",
      "960\n",
      "1080\n",
      "[2024-04-09 20:42:19,419: INFO: 2024372759: Batch 8 uploaded:]\n",
      "1080\n",
      "1200\n",
      "[2024-04-09 20:42:21,926: INFO: 2024372759: Batch 9 uploaded:]\n",
      "1200\n",
      "1320\n",
      "[2024-04-09 20:42:23,197: INFO: 2024372759: Batch 10 uploaded:]\n",
      "1320\n",
      "1440\n",
      "[2024-04-09 20:42:24,448: INFO: 2024372759: Batch 11 uploaded:]\n",
      "1440\n",
      "1560\n",
      "[2024-04-09 20:42:25,720: INFO: 2024372759: Batch 12 uploaded:]\n",
      "1560\n",
      "1680\n",
      "[2024-04-09 20:42:26,976: INFO: 2024372759: Batch 13 uploaded:]\n",
      "1680\n",
      "1800\n",
      "[2024-04-09 20:42:28,258: INFO: 2024372759: Batch 14 uploaded:]\n",
      "1800\n",
      "1920\n",
      "[2024-04-09 20:42:29,725: INFO: 2024372759: Batch 15 uploaded:]\n",
      "1920\n",
      "2040\n",
      "[2024-04-09 20:42:30,984: INFO: 2024372759: Batch 16 uploaded:]\n",
      "2040\n",
      "2160\n",
      "[2024-04-09 20:42:32,282: INFO: 2024372759: Batch 17 uploaded:]\n",
      "2160\n",
      "2280\n",
      "[2024-04-09 20:42:33,584: INFO: 2024372759: Batch 18 uploaded:]\n",
      "2280\n",
      "2400\n",
      "[2024-04-09 20:42:34,850: INFO: 2024372759: Batch 19 uploaded:]\n",
      "2400\n",
      "2520\n",
      "[2024-04-09 20:42:36,120: INFO: 2024372759: Batch 20 uploaded:]\n",
      "2520\n",
      "2640\n",
      "[2024-04-09 20:42:37,366: INFO: 2024372759: Batch 21 uploaded:]\n",
      "2640\n",
      "2760\n",
      "[2024-04-09 20:42:38,627: INFO: 2024372759: Batch 22 uploaded:]\n",
      "2760\n",
      "2880\n",
      "[2024-04-09 20:42:39,879: INFO: 2024372759: Batch 23 uploaded:]\n",
      "2880\n",
      "3000\n",
      "[2024-04-09 20:42:41,141: INFO: 2024372759: Batch 24 uploaded:]\n",
      "3000\n",
      "3120\n",
      "[2024-04-09 20:42:42,442: INFO: 2024372759: Batch 25 uploaded:]\n",
      "3120\n",
      "3240\n",
      "[2024-04-09 20:42:43,708: INFO: 2024372759: Batch 26 uploaded:]\n",
      "3240\n",
      "3360\n",
      "[2024-04-09 20:42:44,993: INFO: 2024372759: Batch 27 uploaded:]\n",
      "3360\n",
      "3480\n",
      "[2024-04-09 20:42:46,242: INFO: 2024372759: Batch 28 uploaded:]\n",
      "3480\n",
      "3600\n",
      "[2024-04-09 20:42:47,500: INFO: 2024372759: Batch 29 uploaded:]\n",
      "3600\n",
      "3720\n",
      "[2024-04-09 20:42:48,762: INFO: 2024372759: Batch 30 uploaded:]\n",
      "3720\n",
      "3840\n",
      "[2024-04-09 20:42:50,038: INFO: 2024372759: Batch 31 uploaded:]\n",
      "3840\n",
      "3960\n",
      "[2024-04-09 20:42:51,302: INFO: 2024372759: Batch 32 uploaded:]\n",
      "3960\n",
      "4080\n",
      "[2024-04-09 20:42:52,557: INFO: 2024372759: Batch 33 uploaded:]\n",
      "4080\n",
      "4200\n",
      "[2024-04-09 20:42:53,816: INFO: 2024372759: Batch 34 uploaded:]\n",
      "4200\n",
      "4320\n",
      "[2024-04-09 20:42:55,088: INFO: 2024372759: Batch 35 uploaded:]\n",
      "4320\n",
      "4440\n",
      "[2024-04-09 20:42:56,333: INFO: 2024372759: Batch 36 uploaded:]\n",
      "4440\n",
      "4560\n",
      "[2024-04-09 20:42:57,610: INFO: 2024372759: Batch 37 uploaded:]\n",
      "4560\n",
      "4680\n",
      "[2024-04-09 20:42:58,871: INFO: 2024372759: Batch 38 uploaded:]\n",
      "4680\n",
      "4800\n",
      "[2024-04-09 20:43:00,162: INFO: 2024372759: Batch 39 uploaded:]\n",
      "4800\n",
      "4920\n",
      "[2024-04-09 20:43:01,418: INFO: 2024372759: Batch 40 uploaded:]\n",
      "4920\n",
      "5040\n",
      "[2024-04-09 20:43:02,712: INFO: 2024372759: Batch 41 uploaded:]\n",
      "5040\n",
      "5160\n",
      "[2024-04-09 20:43:03,979: INFO: 2024372759: Batch 42 uploaded:]\n",
      "5160\n",
      "5280\n",
      "[2024-04-09 20:43:05,224: INFO: 2024372759: Batch 43 uploaded:]\n",
      "5280\n",
      "5400\n",
      "[2024-04-09 20:43:06,511: INFO: 2024372759: Batch 44 uploaded:]\n",
      "5400\n",
      "5520\n",
      "[2024-04-09 20:43:07,794: INFO: 2024372759: Batch 45 uploaded:]\n",
      "5520\n",
      "5640\n",
      "[2024-04-09 20:43:09,068: INFO: 2024372759: Batch 46 uploaded:]\n",
      "5640\n",
      "5760\n",
      "[2024-04-09 20:43:10,338: INFO: 2024372759: Batch 47 uploaded:]\n",
      "5760\n",
      "5880\n",
      "[2024-04-09 20:43:11,630: INFO: 2024372759: Batch 48 uploaded:]\n",
      "5880\n",
      "6000\n",
      "[2024-04-09 20:43:12,897: INFO: 2024372759: Batch 49 uploaded:]\n",
      "6000\n",
      "6120\n",
      "[2024-04-09 20:43:14,149: INFO: 2024372759: Batch 50 uploaded:]\n",
      "6120\n",
      "6124\n",
      "[2024-04-09 20:43:14,379: INFO: 2024372759: Batch 51 uploaded:]\n",
      "[2024-04-09 20:43:29,558: INFO: 2024372759: {'dimension': 1536,\n",
      " 'index_fullness': 0.06,\n",
      " 'namespaces': {'': {'vector_count': 6000}},\n",
      " 'total_vector_count': 6000}:]\n"
     ]
    }
   ],
   "source": [
    "# __main__ section\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        config = ConfigurationManager()\n",
    "        data_upload_config = config.get_data_upload_config()\n",
    "        \n",
    "        data_upload = DataUpload(config=data_upload_config)\n",
    "        data_upload.del_index()\n",
    "        data_upload.recreate_index()\n",
    "        pinecone_vector = data_upload.pinecon_vector()\n",
    "        data_upload.batch_upload(pinecone_vector)\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
