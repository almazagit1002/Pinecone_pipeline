{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Graph_structure_research\"\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files/Graphviz/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Maza\\\\Desktop\\\\Pinecone_pipeline'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Maza\\\\Desktop\\\\Pinecone_pipeline'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class GraphStructureConfig:\n",
    "    root_dir: Path\n",
    "    graph_structure_file: Path\n",
    "    sructure_file: Path\n",
    "    graph_json_model:Path\n",
    "    models: dict\n",
    "    graph_prompts: dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_txt(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "              content = file.read()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_groq import ChatGroq\n",
    "from vector_db_pipeline.constants import *\n",
    "from vector_db_pipeline.utils.common import read_yaml, save_json, create_directories\n",
    "from vector_db_pipeline import logger\n",
    "from dotenv import load_dotenv\n",
    "from graphviz import Digraph\n",
    "import json\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        models_filepath = MODELS_FILE_PATH,\n",
    "        prompt_template = PROMPT_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.models = read_yaml(models_filepath)\n",
    "        self.prompt_template = read_yaml(prompt_template)\n",
    "    \n",
    "        \n",
    "\n",
    "    \n",
    "    def get_graph_structure_config(self) -> GraphStructureConfig:\n",
    "        config = self.config.graph_structure\n",
    "        prompt_teplates = self.prompt_template.generate_graph_structure\n",
    "        create_directories([config.root_dir])\n",
    "        \n",
    "        graph_structure_config = GraphStructureConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            graph_structure_file = config.graph_structure_file,\n",
    "            sructure_file = config.sructure_file,\n",
    "            graph_json_model = config.graph_json_model,\n",
    "            models = self.models,\n",
    "            graph_prompts = prompt_teplates\n",
    "        ) \n",
    "\n",
    "        return graph_structure_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphStructure:\n",
    "    def __init__(self, config:GraphStructureConfig):\n",
    "        self.config = config\n",
    "    def generate_graph_structure(self):\n",
    "        prompt = ChatPromptTemplate.from_messages([(\n",
    "                    \"system\",\n",
    "                        self.config.graph_prompts.system,),\n",
    "                    MessagesPlaceholder(variable_name=\"messages\"),])\n",
    "        models = read_yaml(MODELS_FILE_PATH)\n",
    "        model = models.Llama3\n",
    "        logger.info(f\"Working with model: {model}\")\n",
    "        llm = ChatGroq(temperature=0, model_name=model)\n",
    "        model_generate = prompt | llm\n",
    "\n",
    "        app_structure = read_txt(self.config.sructure_file)\n",
    "        logger.info(f\"App structure loaded\")\n",
    "        task_description = self.config.graph_prompts.task\n",
    "        request = HumanMessage(\n",
    "            content= task_description + app_structure\n",
    "        )\n",
    "        logger.info(f\"Invoking {model}\")\n",
    "        \n",
    "        results = model_generate.invoke({\"messages\": [request]})\n",
    "        logger.info(f\"{model} finished task\")\n",
    "        try:\n",
    "            results_dict = json.loads(results.content)\n",
    "        except Exception as e:\n",
    "            logger.info(f\"Error while formatting structure: {e}\")\n",
    "            \n",
    "            prompt = ChatPromptTemplate.from_messages([(\n",
    "                    \"system\",\n",
    "                        self.config.graph_prompts.system_debugger,),\n",
    "                    MessagesPlaceholder(variable_name=\"messages\"),])\n",
    "            \n",
    "            models = read_yaml(MODELS_FILE_PATH)\n",
    "            model = models.Llama3\n",
    "\n",
    "            llm = ChatGroq(temperature=0, model_name=model)\n",
    "            model_generate = prompt | llm\n",
    "\n",
    "            task_description = self.config.graph_prompts.task_debugger.format(error = str(e), results=results.content)\n",
    "            request = HumanMessage(content= task_description )\n",
    "\n",
    "            results = model_generate.invoke({\"messages\": [request]})\n",
    "            results_dict= json.loads(results.content)\n",
    "            logger.info(f\"Error {e} fixed\")\n",
    "           \n",
    "        \n",
    "        save_json(Path(self.config.graph_json_model),results_dict)\n",
    "        return results_dict \n",
    "    \n",
    "    def generate_graph_viz_and_render(self,results):\n",
    "        try:\n",
    "            filename = self.config.graph_structure_file\n",
    "            model = eval(results)\n",
    "\n",
    "        \n",
    "            graph = Digraph(engine='neato',node_attr={'shape': 'Mrecord'}, format=\"png\")\n",
    "\n",
    "            for c in model['entities']:\n",
    "                attnames = (x['name'] + \"\\n(sch:\" + x['schema_org_term'][18:] + \") \" for x in c['attributes'])\n",
    "                graph.node(c['name'].replace(\" \",\"_\")+\"_node\",   \"{ Entity: \" + c['name']\n",
    "                            + \"\\n(sch:\" + c['schema_org_term'][18:] + \") \" + \"|\" + \"|\".join(attnames) + \"}\")   \n",
    "\n",
    "            for r in model['relationships']:\n",
    "                graph.edge(r['from'].replace(\" \",\"_\") + \"_node\",r['to'].replace(\" \",\"_\") + \"_node\",\n",
    "                            label=r['name'] + \"\\n(sch:\" + r['schema_org_term'][18:] + \") \" , len='6.00')\n",
    "\n",
    "            graph.render(filename)\n",
    "            logger.info(f\"Graph model saved in  {filename}\")\n",
    "            logger.info(f\"Graph image saved in  {filename+'.png'}\")\n",
    "        except Exception as e:\n",
    "            logger.info(f\"Error while generating graph: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-25 20:45:24,137: INFO: common: yaml file: config\\config.yaml loaded successfully:]\n",
      "[2024-04-25 20:45:24,137: INFO: common: yaml file: schema.yaml loaded successfully:]\n",
      "[2024-04-25 20:45:24,143: INFO: common: yaml file: params.yaml loaded successfully:]\n",
      "[2024-04-25 20:45:24,145: INFO: common: yaml file: models.yaml loaded successfully:]\n",
      "[2024-04-25 20:45:24,145: INFO: common: yaml file: prompt_template.yaml loaded successfully:]\n",
      "[2024-04-25 20:45:24,145: INFO: common: Directory already exists: artifacts/graph_structure:]\n",
      "[2024-04-25 20:45:24,145: INFO: common: yaml file: models.yaml loaded successfully:]\n",
      "[2024-04-25 20:45:24,145: INFO: 2119797541: Working with model: llama3-70b-8192:]\n",
      "[2024-04-25 20:45:24,627: INFO: 2119797541: App structure loaded:]\n",
      "[2024-04-25 20:45:24,642: INFO: 2119797541: Invoking llama3-70b-8192:]\n",
      "[2024-04-25 20:45:36,309: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-04-25 20:45:36,325: INFO: 2119797541: llama3-70b-8192 finished task:]\n",
      "[2024-04-25 20:45:36,325: INFO: 2119797541: Error while formatting structure: Expecting ',' delimiter: line 32 column 16 (char 3119):]\n",
      "[2024-04-25 20:45:36,325: INFO: common: yaml file: models.yaml loaded successfully:]\n",
      "[2024-04-25 20:45:42,635: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\":]\n",
      "[2024-04-25 20:45:42,635: INFO: 2119797541: Error Expecting ',' delimiter: line 32 column 16 (char 3119) fixed:]\n",
      "[2024-04-25 20:45:42,635: INFO: common: json file saved at: artifacts\\graph_structure\\graph_json_model.json:]\n",
      "[2024-04-25 20:45:42,833: INFO: 2119797541: Graph model saved in  artifacts/graph_structure/graph_model:]\n",
      "[2024-04-25 20:45:42,833: INFO: 2119797541: Graph image saved in  artifacts/graph_structure/graph_model.png:]\n"
     ]
    }
   ],
   "source": [
    "config = ConfigurationManager()\n",
    "graph_structure_config = config.get_graph_structure_config()\n",
    "graph_structure = GraphStructure(config=graph_structure_config)\n",
    "generate_graph_structure = graph_structure.generate_graph_structure()\n",
    "graph_structure.generate_graph_viz_and_render(str(generate_graph_structure))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
