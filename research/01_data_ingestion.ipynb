{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from vector_db_pipeline.utils.common import list_files_in_directory, get_json\n",
    "from pathlib import Path\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Maza\\\\Desktop\\\\Pinecone_pipeline\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataIngestionConfig:\n",
    "    root_dir: Path\n",
    "    local_data_file: Path\n",
    "    load_dir: Path\n",
    "    text_spliter_config : dict\n",
    "    namespace_idx:str\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vector_db_pipeline.constants import *\n",
    "from vector_db_pipeline.utils.common import read_yaml, create_directories, save_json\n",
    "from vector_db_pipeline import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath=CONFIG_FILE_PATH,\n",
    "        schema_filepath=SCHEMA_FILE_PATH,\n",
    "        params_filepath=PARAMS_FILE_PATH):\n",
    "        \"\"\"\n",
    "        Initializes ConfigurationManager with provided filepaths.\n",
    "\n",
    "        Args:\n",
    "            config_filepath (str): Filepath to configuration file. Defaults to CONFIG_FILE_PATH.\n",
    "            schema_filepath (str): Filepath to schema file. Defaults to SCHEMA_FILE_PATH.\n",
    "            params_filepath (str): Filepath to parameters file. Defaults to PARAMS_FILE_PATH.\n",
    "        \"\"\"\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "    \n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        \"\"\"\n",
    "        Retrieves data ingestion configuration settings.\n",
    "\n",
    "        Returns:\n",
    "            data_ingestion_config (DataIngestionConfig): Data ingestion configuration object.\n",
    "        \"\"\"\n",
    "        config = self.config.data_ingestion\n",
    "        text_spliter = self.params.TEXT_SPLITER\n",
    "        namespace = self.params.INDEX_INFO.NAMESPACE\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            local_data_file=config.local_data_file,\n",
    "            load_dir=config.load_dir,\n",
    "            text_spliter_config=text_spliter,\n",
    "            namespace_idx = namespace\n",
    "        )\n",
    "\n",
    "        return data_ingestion_config\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor:\n",
    "    def __init__(self, config: DataIngestionConfig):\n",
    "        \"\"\"\n",
    "        Initializes TextProcessor with the provided data ingestion configuration.\n",
    "\n",
    "        Args:\n",
    "            config (DataIngestionConfig): Configuration object containing text splitting settings.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        \n",
    "    def get_text_chunks(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Splits input text into chunks based on configuration settings.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text to be split into chunks.\n",
    "\n",
    "        Returns:\n",
    "            chunks (List[str]): List of text chunks.\n",
    "        \"\"\"\n",
    "        text_splitter_config = self.config.text_spliter_config\n",
    "        text_splitter = CharacterTextSplitter(\n",
    "            separator=text_splitter_config.SEPARATOR.encode().decode('unicode_escape'),\n",
    "            chunk_size=text_splitter_config.CHUNK_SIZE,\n",
    "            chunk_overlap=text_splitter_config.CHUNK_OVERLAP,\n",
    "            length_function=len\n",
    "        )\n",
    "        chunks = text_splitter.split_text(text)\n",
    "        return chunks\n",
    "\n",
    "    def split_text(self, data: List[dict]) -> List[dict]:\n",
    "        \"\"\"\n",
    "        Splits text data in each dictionary entry into chunks and embeds each chunk.\n",
    "\n",
    "        Args:\n",
    "            data (List[dict]): List of dictionaries containing text data.\n",
    "\n",
    "        Returns:\n",
    "            splited_text_data (List[dict]): List of dictionaries containing split and embedded text data.\n",
    "        \"\"\"\n",
    "\n",
    "        namespace = self.config.namespace_idx\n",
    "        embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "        splited_text_data = []\n",
    "        idx = 0\n",
    "        for d in data:\n",
    "            # Start schema extraction\n",
    "            text = d.get('text')\n",
    "            if text:\n",
    "                timestamp = d.pop('date_scraped_timestamp')\n",
    "                host = d.pop('host')\n",
    "                url = d.pop('url')\n",
    "                page_title = d.pop('page_title')\n",
    "                # End schema extraction\n",
    "                \n",
    "                text_chunks = self.get_text_chunks(text)\n",
    "                embeded_text = embed_model.embed_documents(text_chunks)\n",
    "                for i, text_chunk in enumerate(text_chunks):\n",
    "                    emb_vect = {'id': namespace+'#'+str(idx), 'values': embeded_text[i], \n",
    "                                'text': text_chunk, 'host': str(host), 'page_title': str(page_title),\n",
    "                                'chunk':str(i), 'url': str(url),'timestamp': str(timestamp)}\n",
    "                    idx += 1\n",
    "                    splited_text_data.append(emb_vect)\n",
    "        logger.info(f\"Text processed and chunked. Total chunks: {len(splited_text_data)}\")\n",
    "        return splited_text_data\n",
    "    \n",
    "    def load_data_json(self, splited_text_data: List[dict]):\n",
    "        \"\"\"\n",
    "        Saves the processed data into a json file.\n",
    "\n",
    "        Args:\n",
    "            splited_text_data (List[dict]): List of dictionaries containing processed text data.\n",
    "        \"\"\"\n",
    "        json_file_path = Path(self.config.load_dir)\n",
    "        df = pd.DataFrame(splited_text_data)\n",
    "        df.to_json(json_file_path, orient='records')\n",
    "\n",
    "        logger.info(f\"Data processed and saved into JSON file in {json_file_path}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    config = ConfigurationManager()\n",
    "    data_ingestion_config = config.get_data_ingestion_config()\n",
    "    json_files = list_files_in_directory(Path(data_ingestion_config.local_data_file))\n",
    "    data = get_json(json_files)\n",
    "    text_processor = TextProcessor(config=data_ingestion_config)\n",
    "    splited_text_data = text_processor.split_text(data[:20])\n",
    "    text_processor.load_data_json(splited_text_data)\n",
    "except Exception as e:\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
