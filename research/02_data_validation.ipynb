{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Maza\\\\Desktop\\\\Pinecone_pipeline'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataValidationConfig:\n",
    "    root_dir: Path\n",
    "    read_data_dir: Path\n",
    "    STATUS_FILE: str\n",
    "    SCHEMA: dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vector_db_pipeline.constants import *\n",
    "from vector_db_pipeline.utils.common import read_yaml, create_directories\n",
    "from vector_db_pipeline import logger\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "        \n",
    "\n",
    "    \n",
    "    def get_data_validation_config(self) -> DataValidationConfig:\n",
    "        config = self.config.data_validation\n",
    "        schema = self.schema.COLUMNS\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_validation_config = DataValidationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            read_data_dir=config.read_data_dir,\n",
    "            STATUS_FILE=config.STATUS_FILE,\n",
    "            SCHEMA=schema\n",
    "        )\n",
    "\n",
    "        return data_validation_config\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from vector_db_pipeline import logger\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataValiadtion:\n",
    "    def __init__(self, config: DataValidationConfig):\n",
    "        self.config = config\n",
    "        self.data = pd.read_json(self.config.read_data_dir, orient='records')\n",
    "\n",
    "\n",
    "    def validate_all_columns(self)-> bool:\n",
    "        try:\n",
    "            validation_status = None\n",
    "\n",
    "            \n",
    "            all_cols = list(self.data.columns)\n",
    "            \n",
    "\n",
    "            all_schema = self.config.SCHEMA\n",
    "            all_schema['id'] = 'str'\n",
    "            all_schema['values'] = 'list'\n",
    "       \n",
    "\n",
    "            \n",
    "            for col in all_cols:\n",
    "                if col not in all_schema.keys():\n",
    "                    validation_status = False\n",
    "                    with open(self.config.STATUS_FILE, 'a') as f:\n",
    "                        f.write(f\"All columns present in data: {validation_status}\\n\")\n",
    "                else:\n",
    "                    validation_status = True\n",
    "                    with open(self.config.STATUS_FILE, 'a') as f:\n",
    "                        f.write(f\"All columns present in data: {validation_status}\\n\")\n",
    "\n",
    "            return logger.info(f\"All columns present in data: {validation_status}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        \n",
    "    def validate_unique_index(self)-> bool:\n",
    "        try:\n",
    "            \n",
    "            unique_id = len(self.data.id.unique()) == len(self.data)\n",
    "            if unique_id:\n",
    "             \n",
    "                with open(self.config.STATUS_FILE, 'a') as f:\n",
    "                    f.write(f\"Unique ids : {unique_id}\\n\")\n",
    "            else:\n",
    "               \n",
    "                with open(self.config.STATUS_FILE, 'a') as f:\n",
    "                    f.write(f\"Unique ids : {unique_id}\\n\")\n",
    "            return logger.info(f\"Unique ids : {unique_id}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        \n",
    "\n",
    "    def validate_column_types(self):\n",
    "        \"\"\"\n",
    "        Get the types of values in each column of the DataFrame.\n",
    "\n",
    "       \n",
    "        Returns:\n",
    "            dict: A dictionary where keys are column names and values are lists containing the types of values present in each column.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            column_types = {}\n",
    "            how_many_types = {}\n",
    "            \n",
    "            # Iterate over each column in the DataFrame\n",
    "            for column in self.data.columns:\n",
    "                # Create a set of unique types for values in the column\n",
    "                types = set(type(value) for value in self.data[column])\n",
    "                # Convert the set to a list for easier handling\n",
    "                types_list = list(types)\n",
    "\n",
    "                column_types[column] = types_list\n",
    "                how_many_types[column] = len(types_list)\n",
    "            \n",
    "            \n",
    "\n",
    "            # Check if elements in 'values' column are of type float\n",
    "            vector_elemts_type = list(set(type(elem) for elem in self.data['values'][0]))\n",
    "            if len(vector_elemts_type) != 1 or vector_elemts_type[0] != float:\n",
    "                raise TypeError(\"Vector values are not of type float\")\n",
    "\n",
    "            # Check if all columns have a single type\n",
    "            unique_types = all(value == 1 for value in how_many_types.values())\n",
    "\n",
    "            # Check if 'id' column is of type string and 'values' column is of type list\n",
    "            id_type = column_types['id'][0] == str\n",
    "            vector_type = column_types['values'][0] == list\n",
    "\n",
    "            if not id_type:\n",
    "                raise TypeError(\"ID column is not of type string\")\n",
    "            elif not vector_type:\n",
    "                raise TypeError(\"Values column is not of type list\")\n",
    "            elif not unique_types:\n",
    "                raise TypeError(\"Several types present in data\")\n",
    "\n",
    "            # Log success message\n",
    "            with open(self.config.STATUS_FILE, 'a') as f:\n",
    "                f.write(\"Data types are correct\\n\")\n",
    "\n",
    "            return logger.info(\"Data types are valid\")\n",
    "        except Exception as e:\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-15 10:48:35,683: INFO: common: yaml file: config\\config.yaml loaded successfully:]\n",
      "[2024-04-15 10:48:35,689: INFO: common: yaml file: schema.yaml loaded successfully:]\n",
      "[2024-04-15 10:48:35,692: INFO: common: Directory already exists: artifacts/data_validation:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'date_scraped_timestamp': 'int', 'host': 'str', 'page_title': 'str', 'text': 'str', 'url': 'str', 'id': 'str', 'values': 'list'}\n",
      "['id', 'values', 'text', 'host', 'page_title', 'chunk', 'url', 'timestamp']\n",
      "[2024-04-15 10:48:36,011: INFO: 1103252934: All columns present in data: False:]\n",
      "[2024-04-15 10:48:36,016: INFO: 1103252934: Unique ids : True:]\n",
      "[2024-04-15 10:48:36,021: INFO: 1103252934: Data types are valid:]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_validation_config = config.get_data_validation_config()\n",
    "    data_validation = DataValiadtion(config=data_validation_config)\n",
    "    data_validation.validate_all_columns()\n",
    "    data_validation.validate_unique_index()\n",
    "    data_validation.get_column_types()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
